{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# get root repository path\n",
    "a = !pwd\n",
    "repo_root = a[0].rsplit('ProGraML', maxsplit=1,)[0] + 'ProGraML'\n",
    "print(repo_root)\n",
    "#insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, repo_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader, InMemoryDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install pytorch-geometric\n",
    "and other packages if needed, following: https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf import text_format\n",
    "from deeplearning.ml4pl.graphs import programl\n",
    "from deeplearning.ml4pl.graphs.labelled import graph_tuple\n",
    "from labm8.py import app\n",
    "\n",
    "#FLAGS = app.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplearning.ml4pl.graphs.unlabelled.llvm2graph import graph_builder\n",
    "\n",
    "builder = graph_builder.ProGraMLGraphBuilder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Tuple Legend\n",
    "\n",
    "\n",
    "#### ```adjacencies: np.array```\n",
    "\n",
    "A list of adjacency lists, one for each flow type, where an entry in an\n",
    "adjacency list is a <src,dst> tuple of node indices.\n",
    "\n",
    "`Shape (edge_flow_count, edge_count, 2), dtype int32`\n",
    "\n",
    "\n",
    "#### ```edge_positions: np.array```\n",
    "\n",
    "A list of edge positions, one for each edge type. An edge position is an\n",
    "integer in the range 0 <= x < edge_position_max.\n",
    "\n",
    "`Shape (edge_flow_count, edge_count), dtype int32`\n",
    "\n",
    "\n",
    "\n",
    "#### ```node_x: np.array```\n",
    "\n",
    "A list of node feature arrays. Each row is a node, and each column is an\n",
    "feature for that node.\n",
    "\n",
    "`Shape (node_count, node_x_dimensionality), dtype int32`\n",
    "\n",
    "\n",
    "#### ```node_y: Optional[np.array] = None```\n",
    "\n",
    "(optional) A list of node labels arrays.\n",
    "\n",
    "`Shape (node_count, node_y_dimensionality), dtype float32`\n",
    "\n",
    "\n",
    "#### ```graph_x: Optional[np.array] = None```\n",
    "\n",
    "(optional) A list of graph features arrays.\n",
    "\n",
    "`Shape (graph_x_dimensionality) OR (graph_count, graph_x_dimensionality) if\n",
    "graph_count > 1, dtype int32`\n",
    "\n",
    "\n",
    "\n",
    "#### ```graph_y: Optional[np.array] = None```\n",
    "\n",
    "(optional) A vector of graph labels arrays.\n",
    "\n",
    "`Shape (graph_y_dimensionality) OR (graph_count, graph_y_dimensionality) if\n",
    "graph_count > 1, dtype float32`\n",
    "\n",
    "\n",
    "## Disjoint graph properties\n",
    "\n",
    "#### ```disjoint_graph_count: int = 1```\n",
    "\n",
    "The number of disconnected graphs in the tuple.\n",
    "\n",
    "\n",
    "#### ```disjoint_nodes_list: np.array = None```\n",
    "\n",
    "A list of integers which segment the nodes by graph. E.g. with a GraphTuple\n",
    "of two distinct graphs, both with three nodes, nodes_list will be\n",
    "[0, 0, 0, 1, 1, 1].\n",
    "`Shape (node_count), dtype int32:`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proto example\n",
    "from google.protobuf import text_format\n",
    "from deeplearning.ml4pl.graphs import programl_pb2 as proto\n",
    "\n",
    "program_graph = proto.ProgramGraph()\n",
    "\n",
    "with open('40.txt_9G8XzpcFlK.programl_proto', 'r') as f:\n",
    "    proto = f.read()\n",
    "\n",
    "proto = text_format.Parse(proto, program_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .ll example\n",
    "with open('71.ll', 'r') as f:\n",
    "    ll = f.read()\n",
    "\n",
    "nx_graph = builder.Build(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tuple example\n",
    "graph_tup = graph_tuple.GraphTuple.CreateFromNetworkX(nx_graph)\n",
    "print([a for a in dir(graph_tup) if '__' not in a])\n",
    "print('\\n')\n",
    "print('edge_positions', [x.shape for x in graph_tup.edge_positions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data example\n",
    "edge_indices = [torch.from_numpy(a).to(dtype=torch.long) for a in graph_tup.adjacencies] # list of <M_i, 2>\n",
    "edge_types = [i * np.ones_like(edge_indices[i])[:,0] for i in range(len(edge_indices))]\n",
    "edge_attr = np.hstack(edge_types)\n",
    "print('edge_attr', edge_attr.shape)\n",
    "\n",
    "t_attr = torch.from_numpy(edge_attr).to(dtype=torch.long).view(-1,1)\n",
    "print(t_attr.size())\n",
    "print('edge_indices', [x.shape for x in edge_indices])\n",
    "print('edge_types', [x.shape for x in edge_types])\n",
    "\n",
    "#edge_pos = [torch.from_numpy(a).to(dtype=torch.long) for a in graph_tup.edge_positions]\n",
    "print('graph_tup.edge_positions', [x.shape for x in graph_tup.edge_positions])\n",
    "edge_pos = np.hstack(graph_tup.edge_positions)\n",
    "\n",
    "edge_attr = np.vstack([edge_attr, edge_pos])\n",
    "\n",
    "edge_attr = torch.from_numpy(edge_attr.T).to(torch.long)\n",
    "print('edge_attr', edge_attr.size())\n",
    "\n",
    "# node_x as data.x\n",
    "x = torch.from_numpy(graph_tup.node_x).to(torch.long) #\n",
    "y = torch.from_numpy(np.array(42)).to(torch.long).view(1)\n",
    "print('x', x.size())\n",
    "print('y', y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple2data(graph_tup, class_label):\n",
    "    # edges as data.edge_index\n",
    "    # list of <M_i, 2> tensors  (M_i = num_edges for ith edge type)\n",
    "    edge_indices = [torch.from_numpy(a).to(dtype=torch.long) for a in graph_tup.adjacencies] # list of <M_i, 2>\n",
    "\n",
    "    edge_index = torch.cat(edge_indices, dim=0).t().contiguous() # <2, M>\n",
    "\n",
    "    # (edge_type, edge_position) as data.edge_attr of shape <M, 2>\n",
    "    edge_types = [i * np.ones_like(edge_indices[i])[:,0] for i in range(len(edge_indices))]\n",
    "    edge_types = np.hstack(edge_types)\n",
    "    edge_pos = np.hstack(graph_tup.edge_positions)\n",
    "    edge_attr = np.vstack([edge_types, edge_pos]).T\n",
    "    \n",
    "    edge_attr = torch.from_numpy(edge_attr).to(dtype=torch.long).contiguous()  # <M, 2>\n",
    "    \n",
    "    assert edge_attr.size()[0] == edge_index.size()[1], f'edge_attr={edge_attr.size()} size mismatch with edge_index={edge_index.size()}'\n",
    "    \n",
    "    # node_x as data.x\n",
    "    x = torch.from_numpy(graph_tup.node_x).to(torch.long)  # <N, 1>\n",
    "    # class label as y\n",
    "    y = torch.from_numpy(np.array(class_label)).to(torch.long).view(1)  # <1>\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple2data example\n",
    "tuple2data(graph_tup, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "def dump(outfile, data, mkdir=True):\n",
    "    if mkdir:\n",
    "        outfile.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(outfile, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ds_raw_dir(ds_base, dump_nx=True, dump_tuple=True, dump_data=True, pool_size=12):\n",
    "    \"\"\"Preprocess all .ll files in folders named by class ids e.g. ds_base/1, ds_base/2, ...\n",
    "    into pytorch-geometric data.Data instances.\n",
    "    \n",
    "    The intermediate nx graphs and graph_tuples can be saved as well.\n",
    "    \"\"\"\n",
    "\n",
    "    out_base = ds_base.parent / (ds_base.name + '_programl')\n",
    "    out_base.mkdir(parents=True, exist_ok=True)\n",
    "    problems = open(out_base / 'problems.txt', 'a')\n",
    "    print(f\"=== DATASET {ds_base}: preprocessing will be saved in {out_base}\")\n",
    "\n",
    "    # get all subfolders 1/ 2/ etc\n",
    "    folders = [x for x in ds_base.glob('*') if x.is_dir()]\n",
    "\n",
    "    for i, folder in enumerate(folders):\n",
    "        try:\n",
    "            int(folder.name)\n",
    "        except ValueError as e:\n",
    "            print(f\"Folder {i} has to be named with integervalues, but is {folder.name}.\")\n",
    "            raise e\n",
    "    \n",
    "    # multiprocessed loop over folders\n",
    "    pool = Pool(processes=pool_size)\n",
    "    task_args = [(folder, dump_nx, dump_tuple, dump_data, out_base) for folder in folders]\n",
    "    \n",
    "    for probs in tqdm.tqdm(pool.imap_unordered(_process_single_folder, task_args), total=len(task_args)):\n",
    "        print(probs, file=problems)\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    problems.close()\n",
    "    print(f\" * COMPLETED * === DATASET {ds_base}: preprocessing saved to {out_base}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _process_single_folder(args):\n",
    "    folder, dump_nx, dump_tuple, dump_data, out_base = args\n",
    "    problems = \"\"\n",
    "    \n",
    "    print(f\"=== Opening Folder {str(folder)} ===\")\n",
    "    \n",
    "    label = int(folder.name)\n",
    "    files = list(folder.glob('*.ll'))\n",
    "    for i, file in enumerate(files):\n",
    "        outfile_nx = out_base / '_nx' / folder.name / (file.name.rsplit('.', 1)[0] + '.nx.p')\n",
    "        outfile_tuple = out_base / '_tuples' / folder.name / (file.name.rsplit('.', 1)[0] + '.tuple.p')\n",
    "        outfile_data = out_base / folder.name / (file.name.rsplit('.', 1)[0] + '.data.p')\n",
    "\n",
    "        # find out where to start processing\n",
    "\n",
    "        # skip entirely?\n",
    "        if outfile_data.is_file():\n",
    "            continue\n",
    "\n",
    "        # start at step 3: tuple -> data?\n",
    "        if outfile_tuple.is_file():\n",
    "            graph_tup = load(outfile_tuple)\n",
    "            \n",
    "            # ~~~ step 3: tuple --> data ~~~\n",
    "            data = tuple2data(graph_tup, class_label=label)\n",
    "            dump(outfile_data, data)\n",
    "            continue\n",
    "\n",
    "        # start at step 2: nx --> tuple ?\n",
    "        if outfile_nx.is_file():\n",
    "            nx_graph = load(outfile)\n",
    "            \n",
    "            # ~~~ step 2: nx --> tuple ~~~\n",
    "            graph_tup = graph_tuple.GraphTuple.CreateFromNetworkX(nx_graph)\n",
    "            if dump_tuple:\n",
    "                dump(outfile_tuple, graph_tup)\n",
    "            \n",
    "            # step 3: tuple --> data            \n",
    "            data = tuple2data(graph_tup, class_label=label)\n",
    "            dump(outfile_data, data)\n",
    "            continue\n",
    "        \n",
    "        # start in the beginning:\n",
    "        # ~~~ step 1: .ll --> nx ~~~\n",
    "        if i % 100 == 0:\n",
    "            print(f\"{folder.name} - [{i}/{len(files)}] Processing {str(file)} ...\")\n",
    "        \n",
    "        with open(file, 'r') as f:\n",
    "            bytecode = f.read()\n",
    "\n",
    "        try:\n",
    "            nx_graph = builder.Build(bytecode) # nx\n",
    "            if dump_nx:\n",
    "                dump(outfile, nx_graph)\n",
    "        except:\n",
    "            print(f\"***** FAILING ON {str(file)} ... renaming file to .ll_ \")\n",
    "            problems += str(file)\n",
    "            problems += '\\n'\n",
    "            file.rename(file.with_suffix('.ll_'))\n",
    "            continue\n",
    "\n",
    "        # step 2: nx --> tuple\n",
    "        graph_tup = graph_tuple.GraphTuple.CreateFromNetworkX(nx_graph)\n",
    "        if dump_tuple:\n",
    "            dump(outfile_tuple, graph_tup)\n",
    "\n",
    "        # step 3: tuple --> data            \n",
    "        data = tuple2data(graph_tup, class_label=label)\n",
    "        dump(outfile_data, data)\n",
    "\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset if needed\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "def download_and_unzip(url, dataset_name, data_folder):\n",
    "    \"\"\"\n",
    "    Download and unzip data set folder from url\n",
    "    :param url: from which to download\n",
    "    :param dataset_name: name of data set (for printing)\n",
    "    :param data_folder: folder in which to put the downloaded data\n",
    "    \"\"\"\n",
    "    print('Downloading', dataset_name, 'data set...')\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    data_zip = wget.download(url, out=data_folder)\n",
    "    print('\\tunzipping...')\n",
    "    zip_ = zipfile.ZipFile(data_zip, 'r')\n",
    "    assert os.path.isdir(data_folder), data_folder\n",
    "    zip_.extractall(data_folder)\n",
    "    zip_.close()\n",
    "    print('\\tdone')\n",
    "\n",
    "def download_classifyapp(dataset_path):\n",
    "    # get Path object\n",
    "    if type(dataset_path) == str:\n",
    "        dataset_path = Path(dataset_path)\n",
    "    dataset_path = dataset_path / 'classifyapp_data'\n",
    "        \n",
    "    # Acquire data\n",
    "    if not dataset_path.exists():\n",
    "        dataset_path.mkdir(parents=True)\n",
    "        download_and_unzip('https://polybox.ethz.ch/index.php/s/JOBjrfmAjOeWCyl/download',\n",
    "                                      'classifyapp_data', str(dataset_path.absolute()))\n",
    "    else:\n",
    "        print(f'skipped downloading to {str(dataset_path.absolute())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *needs action:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls $(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set where to store the dataset and download automagically\n",
    "#ds_basepath = Path('/mnt/data/llvm/master_thesis_datasets')\n",
    "ds_basepath = Path(repo_root) / 'deeplearning/ml4pl/poj104'\n",
    "logs_basepath = ds_basepath / 'classifyapp_logs'\n",
    "\n",
    "ds_basepath.mkdir(parents=True, exist_ok=True)\n",
    "logs_basepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "download_classifyapp(ds_basepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link those places into poj104 folder\n",
    "data_source = str((ds_basepath / 'classifyapp_data').absolute())\n",
    "print(data_source)\n",
    "data_target = repo_root + '/deeplearning/ml4pl/poj104/classifyapp_data'\n",
    "print(data_target)\n",
    "\n",
    "logs_source = str(logs_basepath.absolute())\n",
    "print(logs_source)\n",
    "logs_target = repo_root + '/deeplearning/ml4pl/poj104/classifyapp_logs'\n",
    "print(logs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ln -s {data_source} {data_target}\n",
    "! ln -s {logs_source} {logs_target}\n",
    "! ls -lah {str(repo_root + '/deeplearning/ml4pl/poj104')} | grep classifyapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start processing the smaller validation dataset\n",
    "dataset_path = ds_basepath / Path('classifyapp_data/ir_val')\n",
    "print(dataset_path.name)\n",
    "\n",
    "preprocess_ds_raw_dir(dataset_path, pool_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train dataset\n",
    "dataset_path = ds_basepath / Path('classifyapp_data/ir_train')\n",
    "print(dataset_path.name)\n",
    "\n",
    "preprocess_ds_raw_dir(dataset_path, pool_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test dataset\n",
    "dataset_path = ds_basepath / Path('classifyapp_data/ir_test')\n",
    "print(dataset_path.name)\n",
    "\n",
    "preprocess_ds_raw_dir(dataset_path, pool_size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Pytorch-Geometric Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive dataloader that works on a list of data.Data instances in memory.\n",
    "\n",
    "def get_dataloader(self, ds_base, config):\n",
    "    ds_base = Path(ds_base) if type(ds_base) is str else ds_base\n",
    "    datalist = []\n",
    "    out_base = ds_base.parent / (ds_base.name + \"_programl\")\n",
    "    print(f\"=== DATASET {ds_base}: getting dataloader\")\n",
    "\n",
    "    folders = [x for x in out_base.glob(\"*\") if x.is_dir() and x.name not in ['_nx', '_tuples']]\n",
    "    for folder in tqdm.tqdm(folders):\n",
    "        # skip classes that are larger than what config says to enable debugging with less data\n",
    "        if int(folder.name) > config.num_classes:\n",
    "            continue\n",
    "        # print(f\"=== Opening Folder {str(folder)} ===\")\n",
    "        for k, file in enumerate(folder.glob(\"*.data.p\")):\n",
    "            # print(f\"{k} - Processing {str(file)} ...\")\n",
    "            with open(file, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            datalist.append(data)\n",
    "    print(f\" * COMPLETED * === DATASET {ds_base}: returning dataloader\")\n",
    "    return DataLoader(datalist, batch_size=config.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better dataloader\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "class POJ104Dataset(InMemoryDataset):\n",
    "    def __init__(self, root, split, transform=None, pre_transform=None):\n",
    "        self.split = split\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "\n",
    "        if split == 'train':\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        elif split == 'val':\n",
    "            self.data, self.slices = torch.load(self.processed_paths[1])\n",
    "        elif split == 'test':\n",
    "            self.data, self.slices = torch.load(self.processed_paths[2])\n",
    "        else:\n",
    "            raise ValueError(f\"split={split} but has to be one of train, val, test \")\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return 'classifyapp_data.zip' #['ir_val', 'ir_val_programl']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['train_data.pt', 'val_data.pt'] #, 'test_data.pt']\n",
    "    \n",
    "    def download(self):\n",
    "        # download to self.raw_dir\n",
    "        print(\"I cannot yet download data automatically.\")\n",
    "    \n",
    "    def process(self):\n",
    "        # hardcoded\n",
    "        num_classes = 104\n",
    "        \n",
    "        # output file\n",
    "        if self.split == 'train':\n",
    "            processed_path = self.processed_paths[0]\n",
    "        elif self.split == 'val':\n",
    "            processed_path = self.processed_paths[1]\n",
    "        else:\n",
    "            processed_path = self.processed_paths[2]\n",
    "\n",
    "        # read data into huge `Data` list.\n",
    "        data_list = []\n",
    "        \n",
    "        ds_base = Path(self.root)\n",
    "        print(f'Creating {self.split} dataset at {str(ds_base)}')\n",
    "        out_base = ds_base / ('ir_' + self.split + '_programl')\n",
    "        assert out_base.exists(), f\"{out_base} doesn't exist!\"\n",
    "        print(f\"=== DATASET {out_base}: Collecting .data.p files into dataset\")\n",
    "\n",
    "        folders = [x for x in out_base.glob(\"*\") if x.is_dir() and x.name not in ['_nx', '_tuples']]\n",
    "        for folder in tqdm.tqdm(folders):\n",
    "            # skip classes that are larger than what config says to enable debugging with less data\n",
    "            if int(folder.name) > num_classes:\n",
    "                continue\n",
    "            for k, file in enumerate(folder.glob(\"*.data.p\")):\n",
    "                with open(file, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                data_list.append(data)\n",
    "            \n",
    "        print(f\" * COMPLETED * === DATASET {out_base}: now pre-filtering...\")\n",
    "        # ...\n",
    "        \n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [d for d in data_list if self.pre_filter(d)]\n",
    "        print(f\" * COMPLETED * === DATASET {out_base}: Completed filtering, now pre_transforming...\")\n",
    "        \n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(d) for d in data_list]\n",
    "        \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), processed_path)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = repo_root + '/deeplearning/ml4pl/poj104/classifyapp_data'\n",
    "print(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {raw_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = POJ104Dataset(root=raw_data_path, split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = POJ104Dataset(root=raw_data_path, split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class POJ104(InMemoryDataset):\n",
    "#    torch_geometric.data.InMemoryDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
