{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation: ProGraML x POJ104 <a class='tocSkip'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Configs\" data-toc-modified-id=\"Configs-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Configs</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Graph-Tuple-Legend\" data-toc-modified-id=\"Graph-Tuple-Legend-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Graph Tuple Legend</a></span><ul class=\"toc-item\"><li><span><a href=\"#adjacencies:-np.array\" data-toc-modified-id=\"adjacencies:-np.array-1.0.1.1\"><span class=\"toc-item-num\">1.0.1.1&nbsp;&nbsp;</span><code>adjacencies: np.array</code></a></span></li><li><span><a href=\"#edge_positions:-np.array\" data-toc-modified-id=\"edge_positions:-np.array-1.0.1.2\"><span class=\"toc-item-num\">1.0.1.2&nbsp;&nbsp;</span><code>edge_positions: np.array</code></a></span></li><li><span><a href=\"#node_x:-np.array\" data-toc-modified-id=\"node_x:-np.array-1.0.1.3\"><span class=\"toc-item-num\">1.0.1.3&nbsp;&nbsp;</span><code>node_x: np.array</code></a></span></li><li><span><a href=\"#node_y:-Optional[np.array]-=-None\" data-toc-modified-id=\"node_y:-Optional[np.array]-=-None-1.0.1.4\"><span class=\"toc-item-num\">1.0.1.4&nbsp;&nbsp;</span><code>node_y: Optional[np.array] = None</code></a></span></li><li><span><a href=\"#graph_x:-Optional[np.array]-=-None\" data-toc-modified-id=\"graph_x:-Optional[np.array]-=-None-1.0.1.5\"><span class=\"toc-item-num\">1.0.1.5&nbsp;&nbsp;</span><code>graph_x: Optional[np.array] = None</code></a></span></li><li><span><a href=\"#graph_y:-Optional[np.array]-=-None\" data-toc-modified-id=\"graph_y:-Optional[np.array]-=-None-1.0.1.6\"><span class=\"toc-item-num\">1.0.1.6&nbsp;&nbsp;</span><code>graph_y: Optional[np.array] = None</code></a></span></li></ul></li><li><span><a href=\"#Disjoint-graph-properties\" data-toc-modified-id=\"Disjoint-graph-properties-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Disjoint graph properties</a></span><ul class=\"toc-item\"><li><span><a href=\"#disjoint_graph_count:-int-=-1\" data-toc-modified-id=\"disjoint_graph_count:-int-=-1-1.0.2.1\"><span class=\"toc-item-num\">1.0.2.1&nbsp;&nbsp;</span><code>disjoint_graph_count: int = 1</code></a></span></li><li><span><a href=\"#disjoint_nodes_list:-np.array-=-None\" data-toc-modified-id=\"disjoint_nodes_list:-np.array-=-None-1.0.2.2\"><span class=\"toc-item-num\">1.0.2.2&nbsp;&nbsp;</span><code>disjoint_nodes_list: np.array = None</code></a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Example-Data-Samples\" data-toc-modified-id=\"Example-Data-Samples-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Example Data Samples</a></span><ul class=\"toc-item\"><li><span><a href=\"#Proto-Example:-(NB:-this-uses-a-proto-with-broken-tokenization)\" data-toc-modified-id=\"Proto-Example:-(NB:-this-uses-a-proto-with-broken-tokenization)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Proto Example: (NB: this uses a proto with broken tokenization)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inspect-common-node-IDs:\" data-toc-modified-id=\"Inspect-common-node-IDs:-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Inspect common node IDs:</a></span></li></ul></li><li><span><a href=\"#Draw-a-sample-graph\" data-toc-modified-id=\"Draw-a-sample-graph-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Draw a sample graph</a></span><ul class=\"toc-item\"><li><span><a href=\"#from-.ll-example\" data-toc-modified-id=\"from-.ll-example-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>from .ll example</a></span></li><li><span><a href=\"#GraphTuple-example\" data-toc-modified-id=\"GraphTuple-example-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>GraphTuple example</a></span></li><li><span><a href=\"#Collect-Token-distribution-stats-on-the-example\" data-toc-modified-id=\"Collect-Token-distribution-stats-on-the-example-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Collect Token distribution stats on the example</a></span></li></ul></li></ul></li><li><span><a href=\"#Main:-Preprocess-Datasets\" data-toc-modified-id=\"Main:-Preprocess-Datasets-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Main: Preprocess Datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dev:-tuple2data---GraphTuple-->-Pyg-data.Data\" data-toc-modified-id=\"Dev:-tuple2data---GraphTuple-->-Pyg-data.Data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Dev: tuple2data - GraphTuple -&gt; Pyg data.Data</a></span></li><li><span><a href=\"#Dev:-nx2data-(more-flexible!)\" data-toc-modified-id=\"Dev:-nx2data-(more-flexible!)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Dev: nx2data (more flexible!)</a></span><ul class=\"toc-item\"><li><span><a href=\"#By-Example\" data-toc-modified-id=\"By-Example-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>By Example</a></span></li><li><span><a href=\"#unit-test-on-this-example\" data-toc-modified-id=\"unit-test-on-this-example-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>unit test on this example</a></span></li><li><span><a href=\"#coalesce-into-nx2data-function\" data-toc-modified-id=\"coalesce-into-nx2data-function-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>coalesce into nx2data function</a></span></li></ul></li><li><span><a href=\"#needs-action:\" data-toc-modified-id=\"needs-action:-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span><em>needs action:</em></a></span><ul class=\"toc-item\"><li><span><a href=\"#Process-validation-set\" data-toc-modified-id=\"Process-validation-set-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Process validation set</a></span></li><li><span><a href=\"#Process-test-set\" data-toc-modified-id=\"Process-test-set-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Process test set</a></span></li><li><span><a href=\"#Process-train-set\" data-toc-modified-id=\"Process-train-set-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Process train set</a></span></li></ul></li></ul></li><li><span><a href=\"#Development-of-Pytorch-Geometric-Dataset\" data-toc-modified-id=\"Development-of-Pytorch-Geometric-Dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Development of Pytorch-Geometric Dataset</a></span></li><li><span><a href=\"#Finished-Dataset\" data-toc-modified-id=\"Finished-Dataset-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Finished Dataset</a></span></li><li><span><a href=\"#TBD:-upcoming-work-section\" data-toc-modified-id=\"TBD:-upcoming-work-section-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>TBD: upcoming work section</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# get root repository path\n",
    "a = !pwd\n",
    "repo_root = a[0].rsplit('ProGraML', maxsplit=1,)[0] + 'ProGraML'\n",
    "print(repo_root)\n",
    "#insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, repo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader, InMemoryDataset\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf import text_format\n",
    "from deeplearning.ml4pl.graphs import programl\n",
    "from deeplearning.ml4pl.graphs.labelled import graph_tuple\n",
    "from labm8.py import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplearning.ml4pl.graphs.unlabelled.llvm2graph import graph_builder\n",
    "\n",
    "builder = graph_builder.ProGraMLGraphBuilder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Tuple Legend\n",
    "\n",
    "\n",
    "#### ```adjacencies: np.array```\n",
    "\n",
    "A list of adjacency lists, one for each flow type, where an entry in an\n",
    "adjacency list is a <src,dst> tuple of node indices.\n",
    "\n",
    "`Shape (edge_flow_count, edge_count, 2), dtype int32`\n",
    "\n",
    "\n",
    "#### ```edge_positions: np.array```\n",
    "\n",
    "A list of edge positions, one for each edge type. An edge position is an\n",
    "integer in the range 0 <= x < edge_position_max.\n",
    "\n",
    "`Shape (edge_flow_count, edge_count), dtype int32`\n",
    "\n",
    "\n",
    "\n",
    "#### ```node_x: np.array```\n",
    "\n",
    "A list of node feature arrays. Each row is a node, and each column is an\n",
    "feature for that node.\n",
    "\n",
    "`Shape (node_count, node_x_dimensionality), dtype int32`\n",
    "\n",
    "\n",
    "#### ```node_y: Optional[np.array] = None```\n",
    "\n",
    "(optional) A list of node labels arrays.\n",
    "\n",
    "`Shape (node_count, node_y_dimensionality), dtype float32`\n",
    "\n",
    "\n",
    "#### ```graph_x: Optional[np.array] = None```\n",
    "\n",
    "(optional) A list of graph features arrays.\n",
    "\n",
    "`Shape (graph_x_dimensionality) OR (graph_count, graph_x_dimensionality) if\n",
    "graph_count > 1, dtype int32`\n",
    "\n",
    "\n",
    "\n",
    "#### ```graph_y: Optional[np.array] = None```\n",
    "\n",
    "(optional) A vector of graph labels arrays.\n",
    "\n",
    "`Shape (graph_y_dimensionality) OR (graph_count, graph_y_dimensionality) if\n",
    "graph_count > 1, dtype float32`\n",
    "\n",
    "\n",
    "### Disjoint graph properties\n",
    "\n",
    "#### ```disjoint_graph_count: int = 1```\n",
    "\n",
    "The number of disconnected graphs in the tuple.\n",
    "\n",
    "\n",
    "#### ```disjoint_nodes_list: np.array = None```\n",
    "\n",
    "A list of integers which segment the nodes by graph. E.g. with a GraphTuple\n",
    "of two distinct graphs, both with three nodes, nodes_list will be\n",
    "[0, 0, 0, 1, 1, 1].\n",
    "`Shape (node_count), dtype int32:`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Data Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Proto Example: (NB: this uses a proto with broken tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# proto example\n",
    "from google.protobuf import text_format\n",
    "from deeplearning.ml4pl.graphs import programl_pb2 as proto\n",
    "\n",
    "program_graph = proto.ProgramGraph()\n",
    "\n",
    "with open('40.txt_9G8XzpcFlK.programl_proto', 'r') as f:\n",
    "    proto = f.read()\n",
    "\n",
    "proto = text_format.Parse(proto, program_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Inspect common node IDs:\n",
    "`\n",
    "unreachable,232\n",
    "ret void,263\n",
    "!UNK,8564\n",
    "!IDENTIFIER, 8565\n",
    "magic/root/ I forgot, 8567\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "for node in proto.node:\n",
    "    x.append(list(node.x)[0])\n",
    "\n",
    "bins = np.bincount(x, minlength=8568)\n",
    "s = np.sum(bins)\n",
    "for i, c in enumerate(bins):\n",
    "    if c:\n",
    "        print(f\"{i}  {c}   {(c*100/s):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Draw a sample graph\n",
    "### from .ll example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('71.ll', 'r') as f:\n",
    "    ll = f.read()\n",
    "\n",
    "nx_graph = builder.Build(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect the edge data manually\n",
    "\n",
    "for src, dest, data in nx_graph.edges(data=True):\n",
    "    print(src, dest, data)\n",
    "    break\n",
    "    #print(n, d)\n",
    "    #print(\"\")\n",
    "    #if n > 200:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inspect the node data manually\n",
    "for n, d in nx_graph.nodes.items():\n",
    "    print(n, d)\n",
    "    print(\"\")\n",
    "    if n > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(26,12))\n",
    "nx.draw_networkx(nx_graph, alpha=0.4)\n",
    "plt.show()\n",
    "\n",
    "# requires graphviz\n",
    "#nx.drawing.nx_agraph.to_agraph(nx_graph).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### GraphTuple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tuple example\n",
    "graph_tup = graph_tuple.GraphTuple.CreateFromNetworkX(nx_graph)\n",
    "print([a for a in dir(graph_tup) if '__' not in a])\n",
    "print('\\n')\n",
    "print('edge_positions', [x.shape for x in graph_tup.edge_positions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Collect Token distribution stats on the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = graph_tup.node_x.T.squeeze()\n",
    "\n",
    "\n",
    "bins = np.bincount(x, minlength=8568)\n",
    "s = np.sum(bins)\n",
    "for i, c in enumerate(bins):\n",
    "    if c:\n",
    "        print(f\"{i}  {c}   {(c*100/s):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main: Preprocess Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dev: tuple2data - GraphTuple -> Pyg data.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data example\n",
    "edge_indices = [torch.from_numpy(a).to(dtype=torch.long) for a in graph_tup.adjacencies] # list of <M_i, 2>\n",
    "edge_types = [i * np.ones_like(edge_indices[i])[:,0] for i in range(len(edge_indices))]\n",
    "edge_attr = np.hstack(edge_types)\n",
    "print('edge_attr', edge_attr.shape)\n",
    "\n",
    "t_attr = torch.from_numpy(edge_attr).to(dtype=torch.long).view(-1,1)\n",
    "print(t_attr.size())\n",
    "print('edge_indices', [x.shape for x in edge_indices])\n",
    "print('edge_types', [x.shape for x in edge_types])\n",
    "\n",
    "#edge_pos = [torch.from_numpy(a).to(dtype=torch.long) for a in graph_tup.edge_positions]\n",
    "print('graph_tup.edge_positions', [x.shape for x in graph_tup.edge_positions])\n",
    "edge_pos = np.hstack(graph_tup.edge_positions)\n",
    "\n",
    "edge_attr = np.vstack([edge_attr, edge_pos])\n",
    "\n",
    "edge_attr = torch.from_numpy(edge_attr.T).to(torch.long)\n",
    "print('edge_attr', edge_attr.size())\n",
    "\n",
    "# node_x as data.x\n",
    "x = torch.from_numpy(graph_tup.node_x).to(torch.long) #\n",
    "y = torch.from_numpy(np.array(42)).to(torch.long).view(1)\n",
    "print('x', x.size())\n",
    "print('y', y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tuple2data(graph_tup, class_label):\n",
    "    # edges as data.edge_index\n",
    "    # list of <M_i, 2> tensors  (M_i = num_edges for ith edge type)\n",
    "    edge_indices = [torch.from_numpy(a).to(dtype=torch.long) for a in graph_tup.adjacencies] # list of <M_i, 2>\n",
    "\n",
    "    edge_index = torch.cat(edge_indices, dim=0).t().contiguous() # <2, M>\n",
    "\n",
    "    # (edge_type, edge_position) as data.edge_attr of shape <M, 2>\n",
    "    edge_types = [i * np.ones_like(edge_indices[i])[:,0] for i in range(len(edge_indices))]\n",
    "    edge_types = np.hstack(edge_types)\n",
    "    edge_pos = np.hstack(graph_tup.edge_positions)\n",
    "    edge_attr = np.vstack([edge_types, edge_pos]).T\n",
    "    \n",
    "    edge_attr = torch.from_numpy(edge_attr).to(dtype=torch.long).contiguous()  # <M, 2>\n",
    "    \n",
    "    assert edge_attr.size()[0] == edge_index.size()[1], f'edge_attr={edge_attr.size()} size mismatch with edge_index={edge_index.size()}'\n",
    "    \n",
    "    # node_x as data.x\n",
    "    x = torch.from_numpy(graph_tup.node_x).to(torch.long)  # <N, 1>\n",
    "    # class label as y\n",
    "    y = torch.from_numpy(np.array(class_label)).to(torch.long).view(1)  # <1>\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tuple2data example\n",
    "tuple2data(graph_tup, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev: nx2data (more flexible!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect format\n",
    "print(list(nx_graph.edges(data=True))[12])\n",
    "list(nx_graph.nodes(data=True))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to \"update\" the nx_graph.node.x:\n",
    "from deeplearning.ml4pl.graphs.unlabelled.llvm2graph.node_encoder import GraphNodeEncoder\n",
    "\n",
    "encoder = GraphNodeEncoder()\n",
    "\n",
    "# this changes the nx_graph in-place!\n",
    "encoder.EncodeNodes(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect edge_index\n",
    "edge_index = torch.tensor(list(nx_graph.edges())).t().contiguous()\n",
    "edge_index.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect edge attr\n",
    "positions = []\n",
    "flows = []\n",
    "for i, (_, _, edge_data) in enumerate(nx_graph.edges(data=True)):\n",
    "    positions.append(edge_data['position'])\n",
    "    flows.append(edge_data['flow'])\n",
    "positions = torch.tensor(positions)\n",
    "flows = torch.tensor(flows)\n",
    "edge_attr = torch.cat([flows, positions]).view(2, -1).t().contiguous()\n",
    "edge_attr.size()b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect node attributes\n",
    "print(list(nx_graph.nodes(data=True))[0], '\\n')\n",
    "print(list(nx_graph.nodes(data=True))[27])\n",
    "\n",
    "\n",
    "types = []\n",
    "xs = []\n",
    "for i, node_data in nx_graph.nodes(data=True):\n",
    "    types.append(node_data['type'])\n",
    "    xs.append(node_data['x'])\n",
    "x = torch.cat([torch.tensor(xs), torch.tensor(xs)]).view(2, -1).t().contiguous()\n",
    "x.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unit test on this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coalesce into nx2data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx2data(nx_graph, class_label):\n",
    "    r\"\"\"Converts a :obj:`networkx.Graph` or :obj:`networkx.DiGraph` to a\n",
    "    :class:`torch_geometric.data.Data` instance.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph or networkx.DiGraph): A networkx graph.\n",
    "    \"\"\"\n",
    "\n",
    "    # make sure the nx_graph is encoded properly (since node.x used to be buggy!)\n",
    "    # encoder = GraphNodeEncoder()\n",
    "    # encoder.EncodeNodes(nx_graph)\n",
    "\n",
    "    # collect edge_index\n",
    "    edge_index = torch.tensor(list(nx_graph.edges())).t().contiguous()\n",
    "\n",
    "    # collect edge_attr\n",
    "    positions = []\n",
    "    flows = []\n",
    "    for i, (_, _, edge_data) in enumerate(nx_graph.edges(data=True)):\n",
    "        positions.append(edge_data['position'])\n",
    "        flows.append(edge_data['flow'])\n",
    "    positions = torch.tensor(positions)\n",
    "    flows = torch.tensor(flows)\n",
    "    edge_attr = torch.cat([flows, positions]).view(2, -1).t().contiguous()\n",
    "    \n",
    "    # collect x\n",
    "    types = []\n",
    "    xs = []\n",
    "    for i, node_data in nx_graph.nodes(data=True):\n",
    "        types.append(node_data['type'])\n",
    "        xs.append(node_data['x'])\n",
    "    x = torch.cat([torch.tensor(xs), torch.tensor(types)]).view(2, -1).t().contiguous()\n",
    "\n",
    "    y = torch.tensor(int(class_label)).view(1)  # <1>\n",
    "    \n",
    "    assert edge_attr.size()[0] == edge_index.size()[1], f'edge_attr={edge_attr.size()} size mismatch with edge_index={edge_index.size()}'\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing \n",
    "with open('71.ll', 'r') as f:\n",
    "    ll = f.read()\n",
    "\n",
    "nx_graph = builder.Build(ll)\n",
    "\n",
    "with torch_geometric.debug():\n",
    "    data = nx2data(nx_graph, 42)\n",
    "\n",
    "graph_tup = graph_tuple.GraphTuple.CreateFromNetworkX(nx_graph)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test comparison on '71.ll'\n",
    "\n",
    "golden_data = tuple2data(graph_tup, class_label=42)\n",
    "print(golden_data)\n",
    "\n",
    "# test edge_index\n",
    "print(np.all(np.sort(golden_data.edge_index.numpy()) == np.sort(data.edge_index.numpy())))\n",
    "\n",
    "# test edge_attr (loose test of plausibility)\n",
    "print(np.all(np.sort(golden_data.edge_attr.numpy()[:,0]) == np.sort(data.edge_attr.numpy()[:,0])))\n",
    "print(np.all(np.sort(golden_data.edge_attr.numpy()[:,1]) == np.sort(data.edge_attr.numpy()[:,1])))\n",
    "\n",
    "# test node_x (can only test x and not type with golden!)\n",
    "golden_x_sorted = np.sort(golden_data.x.numpy()[:,0])\n",
    "x_sorted = np.sort(data.x.numpy()[:,0])\n",
    "print(np.all(golden_x_sorted == x_sorted))\n",
    "if not np.all(golden_x_sorted == x_sorted):\n",
    "    print('-------')\n",
    "    mask = golden_x_sorted != x_sorted\n",
    "    print(f'golden_x_sorted[mask] =  {golden_x_sorted[mask]}')\n",
    "    print(f'x_sorted[mask] =         {x_sorted[mask]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "def dump(outfile, data, mkdir=True):\n",
    "    if mkdir:\n",
    "        outfile.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(outfile, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def preprocess_ds_raw_dir(ds_base, dump_nx=True, dump_tuple=True, dump_data=True, pool_size=12):\n",
    "    \"\"\"Preprocess all .ll files in folders named by class ids e.g. ds_base/1, ds_base/2, ...\n",
    "    into pytorch-geometric data.Data instances.\n",
    "    \n",
    "    The intermediate nx graphs and graph_tuples can be saved as well.\n",
    "    \"\"\"\n",
    "\n",
    "    out_base = ds_base.parent / (ds_base.name + '_programl')\n",
    "    out_base.mkdir(parents=True, exist_ok=True)\n",
    "    problems = open(out_base / 'problems.txt', 'a')\n",
    "    print(f\"=== DATASET {ds_base}: preprocessing will be saved in {out_base}\")\n",
    "\n",
    "    # get all subfolders 1/ 2/ etc\n",
    "    folders = [x for x in ds_base.glob('*') if x.is_dir()]\n",
    "\n",
    "    for i, folder in enumerate(folders):\n",
    "        try:\n",
    "            int(folder.name)\n",
    "        except ValueError as e:\n",
    "            print(f\"Folder {i} has to be named with integervalues, but is {folder.name}.\")\n",
    "            raise e\n",
    "    \n",
    "    # multiprocessed loop over folders\n",
    "    pool = Pool(processes=pool_size)\n",
    "    task_args = [(folder, dump_nx, dump_tuple, dump_data, out_base) for folder in folders]\n",
    "    \n",
    "    for probs in tqdm.tqdm(pool.imap_unordered(_process_single_folder, task_args), total=len(task_args)):\n",
    "        if len(probs) > 15: # don't print empty strings like '\\n\\n\\n'\n",
    "            print(probs, file=problems)\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    problems.close()\n",
    "    print(f\" * COMPLETED * === DATASET {ds_base}: preprocessing saved to {out_base}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def _process_single_folder(args):\n",
    "    \"\"\"The new version will skip tuple creation completely.\"\"\"\n",
    "    folder, dump_nx, dump_tuple, dump_data, out_base = args\n",
    "    del dump_tuple\n",
    "    \n",
    "    problems = \"\"\n",
    "    \n",
    "    print(f\"=== Opening Folder {str(folder)} ===\")\n",
    "    \n",
    "    label = int(folder.name)\n",
    "    files = list(folder.glob('*.ll'))\n",
    "    \n",
    "    # iterate over all .ll files in folder and confirm and respectively create the .nx.p and .data.p files\n",
    "    for i, file in enumerate(files):\n",
    "        outfile_nx = out_base / '_nx' / folder.name / (file.name.rsplit('.', 1)[0] + '.nx.p')\n",
    "        outfile_data = out_base / folder.name / (file.name.rsplit('.', 1)[0] + '.data.p')\n",
    "\n",
    "        # find out where to start processing\n",
    "\n",
    "        # skip entirely?\n",
    "        if outfile_data.is_file():\n",
    "            continue\n",
    "\n",
    "        # start at step 2: nx --> data ?\n",
    "        if outfile_nx.is_file():\n",
    "            nx_graph = load(outfile_nx)            \n",
    "            data = nx2data(nx_graph, class_label=label)\n",
    "            dump(outfile_data, data)\n",
    "            \n",
    "            # TODO: delete this line soon !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            # also write back nx_file because the tokenization has been bug-fixed!\n",
    "            # dump(outfile_nx, nx_graph)\n",
    "            continue\n",
    "        \n",
    "        # start in the beginning:\n",
    "        # ~~~ step 1: .ll --> nx ~~~\n",
    "        if i % 100 == 0:\n",
    "            print(f\"{folder.name} - [{i}/{len(files)}] Processing {str(file)} ...\")\n",
    "        \n",
    "        with open(file, 'r') as f:\n",
    "            bytecode = f.read()\n",
    "\n",
    "        try:\n",
    "            nx_graph = builder.Build(bytecode) # nx\n",
    "            if dump_nx:\n",
    "                dump(outfile_nx, nx_graph)\n",
    "        except:\n",
    "            print(f\"***** FAILING ON {str(file)} ... renaming file to .ll_ \")\n",
    "            problems += str(file)\n",
    "            problems += '\\n'\n",
    "            file.rename(file.with_suffix('.ll_'))\n",
    "            continue\n",
    "\n",
    "        # step 2: nx --> data\n",
    "        data = nx2data(nx_graph, class_label=label)\n",
    "        dump(outfile_data, data)\n",
    "\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _process_single_folder_deprecated(args):\n",
    "    folder, dump_nx, dump_tuple, dump_data, out_base = args\n",
    "    problems = \"\"\n",
    "    \n",
    "    print(f\"=== Opening Folder {str(folder)} ===\")\n",
    "    \n",
    "    label = int(folder.name)\n",
    "    files = list(folder.glob('*.ll'))\n",
    "    for i, file in enumerate(files):\n",
    "        outfile_nx = out_base / '_nx' / folder.name / (file.name.rsplit('.', 1)[0] + '.nx.p')\n",
    "        outfile_tuple = out_base / '_tuples' / folder.name / (file.name.rsplit('.', 1)[0] + '.tuple.p')\n",
    "        outfile_data = out_base / folder.name / (file.name.rsplit('.', 1)[0] + '.data.p')\n",
    "\n",
    "        # find out where to start processing\n",
    "\n",
    "        # skip entirely?\n",
    "        if outfile_data.is_file():\n",
    "            continue\n",
    "\n",
    "        # start at step 3: tuple -> data?\n",
    "        if outfile_tuple.is_file():\n",
    "            graph_tup = load(outfile_tuple)\n",
    "            \n",
    "            # ~~~ step 3: tuple --> data ~~~\n",
    "            data = tuple2data(graph_tup, class_label=label)\n",
    "            dump(outfile_data, data)\n",
    "            continue\n",
    "\n",
    "        # start at step 2: nx --> tuple ?\n",
    "        if outfile_nx.is_file():\n",
    "            nx_graph = load(outfile)\n",
    "            \n",
    "            # ~~~ step 2: nx --> tuple ~~~\n",
    "            graph_tup = graph_tuple.GraphTuple.CreateFromNetworkX(nx_graph)\n",
    "            if dump_tuple:\n",
    "                dump(outfile_tuple, graph_tup)\n",
    "            \n",
    "            # step 3: tuple --> data            \n",
    "            data = tuple2data(graph_tup, class_label=label)\n",
    "            dump(outfile_data, data)\n",
    "            continue\n",
    "        \n",
    "        # start in the beginning:\n",
    "        # ~~~ step 1: .ll --> nx ~~~\n",
    "        if i % 100 == 0:\n",
    "            print(f\"{folder.name} - [{i}/{len(files)}] Processing {str(file)} ...\")\n",
    "        \n",
    "        with open(file, 'r') as f:\n",
    "            bytecode = f.read()\n",
    "\n",
    "        try:\n",
    "            nx_graph = builder.Build(bytecode) # nx\n",
    "            if dump_nx:\n",
    "                dump(outfile, nx_graph)\n",
    "        except:\n",
    "            print(f\"***** FAILING ON {str(file)} ... renaming file to .ll_ \")\n",
    "            problems += str(file)\n",
    "            problems += '\\n'\n",
    "            file.rename(file.with_suffix('.ll_'))\n",
    "            continue\n",
    "\n",
    "        # step 2: nx --> tuple\n",
    "        graph_tup = graph_tuple.GraphTuple.CreateFromNetworkX(nx_graph)\n",
    "        if dump_tuple:\n",
    "            dump(outfile_tuple, graph_tup)\n",
    "\n",
    "        # step 3: tuple --> data            \n",
    "        data = tuple2data(graph_tup, class_label=label)\n",
    "        dump(outfile_data, data)\n",
    "\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# download dataset if needed\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "def download_and_unzip(url, dataset_name, data_folder):\n",
    "    \"\"\"\n",
    "    Download and unzip data set folder from url\n",
    "    :param url: from which to download\n",
    "    :param dataset_name: name of data set (for printing)\n",
    "    :param data_folder: folder in which to put the downloaded data\n",
    "    \"\"\"\n",
    "    print('Downloading', dataset_name, 'data set...')\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    data_zip = wget.download(url, out=data_folder)\n",
    "    print('\\tunzipping...')\n",
    "    zip_ = zipfile.ZipFile(data_zip, 'r')\n",
    "    assert os.path.isdir(data_folder), data_folder\n",
    "    zip_.extractall(data_folder)\n",
    "    zip_.close()\n",
    "    print('\\tdone')\n",
    "\n",
    "def download_classifyapp(dataset_path):\n",
    "    # get Path object\n",
    "    if type(dataset_path) == str:\n",
    "        dataset_path = Path(dataset_path)\n",
    "    dataset_path = dataset_path / 'classifyapp_data'\n",
    "        \n",
    "    # Acquire data\n",
    "    if not dataset_path.exists():\n",
    "        dataset_path.mkdir(parents=True)\n",
    "        download_and_unzip('https://polybox.ethz.ch/index.php/s/JOBjrfmAjOeWCyl/download',\n",
    "                                      'classifyapp_data', str(dataset_path.absolute()))\n",
    "    else:\n",
    "        print(f'skipped downloading to {str(dataset_path.absolute())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *needs action:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set where to store the dataset and download automagically\n",
    "ds_basepath = Path('/mnt/data/llvm/master_thesis_datasets')\n",
    "\n",
    "# uncomment this line to save data \"in place\"\n",
    "#ds_basepath = Path(repo_root) / 'deeplearning/ml4pl/poj104'\n",
    "\n",
    "logs_basepath = ds_basepath / 'classifyapp_logs'\n",
    "\n",
    "ds_basepath.mkdir(parents=True, exist_ok=True)\n",
    "logs_basepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "download_classifyapp(ds_basepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link those places into poj104 folder\n",
    "\n",
    "data_source = str((ds_basepath / 'classifyapp_data').absolute())\n",
    "print(data_source)\n",
    "data_target = repo_root + '/deeplearning/ml4pl/poj104/'\n",
    "print(data_target)\n",
    "\n",
    "logs_source = str(logs_basepath.absolute())\n",
    "print(logs_source)\n",
    "logs_target = repo_root + '/deeplearning/ml4pl/poj104/'\n",
    "print(logs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ln -s {data_source} {data_target}\n",
    "! ln -s {logs_source} {logs_target}\n",
    "! ls -lah {str(repo_root + '/deeplearning/ml4pl/poj104')} | grep classifyapp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start processing the smaller validation dataset\n",
    "dataset_path = ds_basepath / Path('classifyapp_data/rerunning_val_test_data/ir_val')\n",
    "print(dataset_path.name)\n",
    "\n",
    "preprocess_ds_raw_dir(dataset_path, pool_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test dataset\n",
    "dataset_path = ds_basepath / Path('classifyapp_data/rerunning_val_test_data/ir_test')\n",
    "print(dataset_path.name)\n",
    "\n",
    "preprocess_ds_raw_dir(dataset_path, pool_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train dataset\n",
    "dataset_path = ds_basepath / Path('classifyapp_data/ir_train')\n",
    "print(dataset_path.name)\n",
    "\n",
    "preprocess_ds_raw_dir(dataset_path, pool_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Development of Pytorch-Geometric Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# naive dataloader that works on a list of data.Data instances in memory.\n",
    "\n",
    "def get_dataloader(self, ds_base, config):\n",
    "    ds_base = Path(ds_base) if type(ds_base) is str else ds_base\n",
    "    datalist = []\n",
    "    out_base = ds_base.parent / (ds_base.name + \"_programl\")\n",
    "    print(f\"=== DATASET {ds_base}: getting dataloader\")\n",
    "\n",
    "    folders = [x for x in out_base.glob(\"*\") if x.is_dir() and x.name not in ['_nx', '_tuples']]\n",
    "    for folder in tqdm.tqdm(folders):\n",
    "        # skip classes that are larger than what config says to enable debugging with less data\n",
    "        if int(folder.name) > config.num_classes:\n",
    "            continue\n",
    "        # print(f\"=== Opening Folder {str(folder)} ===\")\n",
    "        for k, file in enumerate(folder.glob(\"*.data.p\")):\n",
    "            # print(f\"{k} - Processing {str(file)} ...\")\n",
    "            with open(file, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            datalist.append(data)\n",
    "    print(f\" * COMPLETED * === DATASET {ds_base}: returning dataloader\")\n",
    "    return DataLoader(datalist, batch_size=config.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Finished Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from deeplearning.ml4pl.poj104.dataset import POJ104Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#raw_data_path = '/mnt/data/llvm/master_thesis_datasets/classifyapp_data/'\n",
    "raw_data_path = repo_root + '/deeplearning/ml4pl/poj104/classifyapp_data'\n",
    "print(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "POJ104Dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# TBD: upcoming work section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "762.997px",
    "width": "260px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
