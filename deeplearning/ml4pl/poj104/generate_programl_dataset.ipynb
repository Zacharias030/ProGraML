{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# get root repository path\n",
    "a = !pwd\n",
    "root = a[0].rsplit('ProGraML', maxsplit=1,)[0] + 'ProGraML'\n",
    "print(root)\n",
    "#insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader, InMemoryDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install pytorch-geometric\n",
    "and other packages if needed, following: https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf import text_format\n",
    "from deeplearning.ml4pl.graphs import programl\n",
    "from deeplearning.ml4pl.graphs.labelled import graph_tuple\n",
    "from labm8.py import app\n",
    "\n",
    "#FLAGS = app.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplearning.ml4pl.graphs.unlabelled.llvm2graph import graph_builder\n",
    "\n",
    "builder = graph_builder.ProGraMLGraphBuilder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Tuple Legend\n",
    "\n",
    "\n",
    "#### ```adjacencies: np.array```\n",
    "\n",
    "A list of adjacency lists, one for each flow type, where an entry in an\n",
    "adjacency list is a <src,dst> tuple of node indices.\n",
    "\n",
    "`Shape (edge_flow_count, edge_count, 2), dtype int32`\n",
    "\n",
    "\n",
    "#### ```edge_positions: np.array```\n",
    "\n",
    "A list of edge positions, one for each edge type. An edge position is an\n",
    "integer in the range 0 <= x < edge_position_max.\n",
    "\n",
    "`Shape (edge_flow_count, edge_count), dtype int32`\n",
    "\n",
    "\n",
    "\n",
    "#### ```node_x: np.array```\n",
    "\n",
    "A list of node feature arrays. Each row is a node, and each column is an\n",
    "feature for that node.\n",
    "\n",
    "`Shape (node_count, node_x_dimensionality), dtype int32`\n",
    "\n",
    "\n",
    "#### ```node_y: Optional[np.array] = None```\n",
    "\n",
    "(optional) A list of node labels arrays.\n",
    "\n",
    "`Shape (node_count, node_y_dimensionality), dtype float32`\n",
    "\n",
    "\n",
    "#### ```graph_x: Optional[np.array] = None```\n",
    "\n",
    "(optional) A list of graph features arrays.\n",
    "\n",
    "`Shape (graph_x_dimensionality) OR (graph_count, graph_x_dimensionality) if\n",
    "graph_count > 1, dtype int32`\n",
    "\n",
    "\n",
    "\n",
    "#### ```graph_y: Optional[np.array] = None```\n",
    "\n",
    "(optional) A vector of graph labels arrays.\n",
    "\n",
    "`Shape (graph_y_dimensionality) OR (graph_count, graph_y_dimensionality) if\n",
    "graph_count > 1, dtype float32`\n",
    "\n",
    "\n",
    "## Disjoint graph properties\n",
    "\n",
    "#### ```disjoint_graph_count: int = 1```\n",
    "\n",
    "The number of disconnected graphs in the tuple.\n",
    "\n",
    "\n",
    "#### ```disjoint_nodes_list: np.array = None```\n",
    "\n",
    "A list of integers which segment the nodes by graph. E.g. with a GraphTuple\n",
    "of two distinct graphs, both with three nodes, nodes_list will be\n",
    "[0, 0, 0, 1, 1, 1].\n",
    "`Shape (node_count), dtype int32:`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proto example\n",
    "from google.protobuf import text_format\n",
    "from deeplearning.ml4pl.graphs import programl_pb2 as proto\n",
    "\n",
    "program_graph = proto.ProgramGraph()\n",
    "\n",
    "with open('40.txt_9G8XzpcFlK.programl_proto', 'r') as f:\n",
    "    proto = f.read()\n",
    "\n",
    "proto = text_format.Parse(proto, program_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .ll example\n",
    "with open('71.ll', 'r') as f:\n",
    "    ll = f.read()\n",
    "\n",
    "nx_graph = builder.Build(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tuple example\n",
    "graph_tup = graph_tuple.GraphTuple.CreateFromNetworkX(nx_graph)\n",
    "print([a for a in dir(graph_tup) if '__' not in a])\n",
    "print('\\n')\n",
    "print('edge_positions', [x.shape for x in graph_tup.edge_positions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data example\n",
    "edge_indices = [torch.from_numpy(a).to(dtype=torch.long) for a in graph_tup.adjacencies] # list of <M_i, 2>\n",
    "edge_types = [i * np.ones_like(edge_indices[i])[:,0] for i in range(len(edge_indices))]\n",
    "edge_attr = np.hstack(edge_types)\n",
    "print('edge_attr', edge_attr.shape)\n",
    "\n",
    "t_attr = torch.from_numpy(edge_attr).to(dtype=torch.long).view(-1,1)\n",
    "print(t_attr.size())\n",
    "print('edge_indices', [x.shape for x in edge_indices])\n",
    "print('edge_types', [x.shape for x in edge_types])\n",
    "\n",
    "#edge_pos = [torch.from_numpy(a).to(dtype=torch.long) for a in graph_tup.edge_positions]\n",
    "print('graph_tup.edge_positions', [x.shape for x in graph_tup.edge_positions])\n",
    "edge_pos = np.hstack(graph_tup.edge_positions)\n",
    "\n",
    "edge_attr = np.vstack([edge_attr, edge_pos])\n",
    "\n",
    "edge_attr = torch.from_numpy(edge_attr.T).to(torch.long)\n",
    "print('edge_attr', edge_attr.size())\n",
    "\n",
    "# node_x as data.x\n",
    "x = torch.from_numpy(graph_tup.node_x).to(torch.long) #\n",
    "y = torch.from_numpy(np.array(42)).to(torch.long).view(1)\n",
    "print('x', x.size())\n",
    "print('y', y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple2data(graph_tup, class_label):\n",
    "    # edges as data.edge_index\n",
    "    # list of <M_i, 2> tensors  (M_i = num_edges for ith edge type)\n",
    "    edge_indices = [torch.from_numpy(a).to(dtype=torch.long) for a in graph_tup.adjacencies] # list of <M_i, 2>\n",
    "\n",
    "    edge_index = torch.cat(edge_indices, dim=0).t().contiguous() # <2, M>\n",
    "\n",
    "    # (edge_type, edge_position) as data.edge_attr of shape <M, 2>\n",
    "    edge_types = [i * np.ones_like(edge_indices[i])[:,0] for i in range(len(edge_indices))]\n",
    "    edge_types = np.hstack(edge_types)\n",
    "    edge_pos = np.hstack(graph_tup.edge_positions)\n",
    "    edge_attr = np.vstack([edge_types, edge_pos]).T\n",
    "    \n",
    "    edge_attr = torch.from_numpy(edge_attr).to(dtype=torch.long).contiguous()  # <M, 2>\n",
    "    \n",
    "    assert edge_attr.size()[0] == edge_index.size()[1], f'edge_attr={edge_attr.size()} size mismatch with edge_index={edge_index.size()}'\n",
    "    \n",
    "    # node_x as data.x\n",
    "    x = torch.from_numpy(graph_tup.node_x).to(torch.long)  # <N, 1>\n",
    "    # class label as y\n",
    "    y = torch.from_numpy(np.array(class_label)).to(torch.long).view(1)  # <1>\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple2data example\n",
    "tuple2data(graph_tup, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "def dump(outfile, data, mkdir=True):\n",
    "    if mkdir:\n",
    "        outfile.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(outfile, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ds_raw_dir(ds_base, dump_nx=True, dump_tuple=True, dump_data=True, pool_size=12):\n",
    "    \"\"\"Preprocess all .ll files in folders named by class ids e.g. ds_base/1, ds_base/2, ...\n",
    "    into pytorch-geometric data.Data instances.\n",
    "    \n",
    "    The intermediate nx graphs and graph_tuples can be saved as well.\n",
    "    \"\"\"\n",
    "\n",
    "    out_base = ds_base.parent / (ds_base.name + '_programl')\n",
    "    out_base.mkdir(parents=True, exist_ok=True)\n",
    "    problems = open(out_base / 'problems.txt', 'a')\n",
    "    print(f\"=== DATASET {ds_base}: preprocessing will be saved in {out_base}\")\n",
    "\n",
    "    # get all subfolders 1/ 2/ etc\n",
    "    folders = [x for x in ds_base.glob('*') if x.is_dir()]\n",
    "\n",
    "    for i, folder in enumerate(folders):\n",
    "        try:\n",
    "            int(folder.name)\n",
    "        except ValueError as e:\n",
    "            print(f\"Folder {i} has to be named with integervalues, but is {folder.name}.\")\n",
    "            raise e\n",
    "    \n",
    "    # multiprocessed loop over folders\n",
    "    pool = Pool(processes=pool_size)\n",
    "    task_args = [(folder, dump_nx, dump_tuple, dump_data, out_base) for folder in folders]\n",
    "    \n",
    "    for probs in tqdm.tqdm(pool.imap(_process_single_folder, task_args), total=len(task_args)):\n",
    "        print(probs, file=problems)\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    problems.close()\n",
    "    print(f\" * COMPLETED * === DATASET {ds_base}: preprocessing saved to {out_base}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _process_single_folder(args):\n",
    "    folder, dump_nx, dump_tuple, dump_data, out_base = args\n",
    "    problems = \"\"\n",
    "    \n",
    "    print(f\"=== Opening Folder {str(folder)} ===\")\n",
    "    \n",
    "    label = int(folder.name)\n",
    "    files = list(folder.glob('*.ll'))\n",
    "    for i, file in enumerate(files):\n",
    "        \n",
    "        # ~~~ step 1: .ll --> nx ~~~\n",
    "        \n",
    "        outfile = out_base / '_nx' / folder.name / (file.name.rsplit('.', 1)[0] + '.nx.p')\n",
    "        if outfile.is_file():\n",
    "            nx_graph = load(outfile)\n",
    "        else:\n",
    "            if i % 100 == 0:\n",
    "                print(f\"{folder.name} - [{i}/{len(files)}] Processing {str(file)} ...\")\n",
    "            with open(file, 'r') as f:\n",
    "                bytecode = f.read()\n",
    "            try:\n",
    "                nx_graph = builder.Build(bytecode) # nx\n",
    "                if dump_nx:\n",
    "                    dump(outfile, nx_graph)\n",
    "            except:\n",
    "                print(f\"***** FAILING ON {str(file)} .................. \")\n",
    "                problems += str(file)\n",
    "                problems += '\\n'\n",
    "                continue\n",
    "\n",
    "\n",
    "        # ~~~ step 2: nx --> tuple ~~~\n",
    "        \n",
    "        outfile = out_base / '_tuples' / folder.name / (file.name.rsplit('.', 1)[0] + '.tuple.p')\n",
    "        if outfile.is_file():\n",
    "            graph_tup = load(outfile)\n",
    "        else:\n",
    "            graph_tup = graph_tuple.GraphTuple.CreateFromNetworkX(nx_graph)\n",
    "            if dump_tuple:\n",
    "                dump(outfile, graph_tup)\n",
    "\n",
    "        # step 3: tuple --> data\n",
    "        outfile = out_base / folder.name / (file.name.rsplit('.', 1)[0] + '.data.p')\n",
    "        if outfile.is_file():\n",
    "            continue\n",
    "        data = tuple2data(graph_tup, class_label=label)\n",
    "        dump(outfile, data)\n",
    "\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset if needed\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "def download_and_unzip(url, dataset_name, data_folder):\n",
    "    \"\"\"\n",
    "    Download and unzip data set folder from url\n",
    "    :param url: from which to download\n",
    "    :param dataset_name: name of data set (for printing)\n",
    "    :param data_folder: folder in which to put the downloaded data\n",
    "    \"\"\"\n",
    "    print('Downloading', dataset_name, 'data set...')\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    data_zip = wget.download(url, out=data_folder)\n",
    "    print('\\tunzipping...')\n",
    "    zip_ = zipfile.ZipFile(data_zip, 'r')\n",
    "    assert os.path.isdir(data_folder), data_folder\n",
    "    zip_.extractall(data_folder)\n",
    "    zip_.close()\n",
    "    print('\\tdone')\n",
    "\n",
    "def download_classifyapp(dataset_path):\n",
    "    # get Path object\n",
    "    if type(dataset_path) == str:\n",
    "        dataset_path = Path(dataset_path)\n",
    "    dataset_path = dataset_path / 'classifyapp_data'\n",
    "        \n",
    "    # Acquire data\n",
    "    if not dataset_path.exists():\n",
    "        dataset_path.mkdir(parents=True)\n",
    "        download_and_unzip('https://polybox.ethz.ch/index.php/s/JOBjrfmAjOeWCyl/download',\n",
    "                                      'classifyapp_data', str(dataset_path.absolute()))\n",
    "    else:\n",
    "        print(f'skipped downloading to {str(dataset_path.absolute())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *needs action:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set where to store the dataset and download automagically\n",
    "ds_basepath = Path('/mnt/data/llvm/master_thesis_datasets')\n",
    "logs_basepath = ds_basepath / 'classifyapp_logs'\n",
    "\n",
    "ds_basepath.mkdir(parents=True, exist_ok=True)\n",
    "logs_basepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "download_classifyapp(ds_basepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link those places into poj104 folder\n",
    "data_source = str((ds_basepath / 'classifyapp_data').absolute())\n",
    "print(data_source)\n",
    "data_target = root + '/deeplearning/ml4pl/poj104/classifyapp_data'\n",
    "print(data_target)\n",
    "\n",
    "logs_source = str(logs_basepath.absolute())\n",
    "print(logs_source)\n",
    "logs_target = root + '/deeplearning/ml4pl/poj104/classifyapp_logs'\n",
    "print(logs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ln -s {data_source} {data_target}\n",
    "! ln -s {logs_source} {logs_target}\n",
    "! ls -lah {str(root + '/deeplearning/ml4pl/poj104')} | grep classifyapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start processing the smaller validation dataset\n",
    "dataset_path = ds_basepath / Path('classifyapp_data/ir_val')\n",
    "print(dataset_path.name)\n",
    "\n",
    "preprocess_ds_raw_dir(dataset_path, pool_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** FAILING ON /mnt/data/llvm/master_thesis_datasets/classifyapp_data/ir_train/2/691.txt_ZUYqeeItxG.ll .................. \n",
      "***** FAILING ON /mnt/data/llvm/master_thesis_datasets/classifyapp_data/ir_train/100/2480.txt_G3q7mLFdGW.ll .................. \n",
      "***** FAILING ON /mnt/data/llvm/master_thesis_datasets/classifyapp_data/ir_train/49/10.txt_O5EX5yuCCN.ll .................. \n"
     ]
    }
   ],
   "source": [
    "# train dataset\n",
    "dataset_path = ds_basepath / Path('classifyapp_data/ir_train')\n",
    "print(dataset_path.name)\n",
    "\n",
    "preprocess_ds_raw_dir(dataset_path, pool_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test dataset\n",
    "dataset_path = ds_basepath / Path('classifyapp_data/ir_test')\n",
    "print(dataset_path.name)\n",
    "\n",
    "preprocess_ds_raw_dir(dataset_path, pool_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class POJ104(InMemoryDataset):\n",
    "#    torch_geometric.data.InMemoryDataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
