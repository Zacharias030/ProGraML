{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation: ProGraML x Threadcoarsening <a class='tocSkip'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Configs\" data-toc-modified-id=\"Configs-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Configs</a></span></li><li><span><a href=\"#Dev:-Helper-Functions\" data-toc-modified-id=\"Dev:-Helper-Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dev: Helper Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dev:-nx2data\" data-toc-modified-id=\"Dev:-nx2data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Dev: nx2data</a></span></li></ul></li><li><span><a href=\"#Main\" data-toc-modified-id=\"Main-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Main</a></span><ul class=\"toc-item\"><li><span><a href=\"#needs-action:\" data-toc-modified-id=\"needs-action:-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span><em>needs action:</em></a></span></li><li><span><a href=\"#Process-.ll-files-(from-NCC-release)\" data-toc-modified-id=\"Process-.ll-files-(from-NCC-release)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Process .ll files (from NCC release)</a></span></li><li><span><a href=\"#fetch-labels-etc.-from-csv\" data-toc-modified-id=\"fetch-labels-etc.-from-csv-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>fetch labels etc. from csv</a></span></li><li><span><a href=\"#Dataset-requirements-for-training:\" data-toc-modified-id=\"Dataset-requirements-for-training:-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Dataset requirements for training:</a></span></li></ul></li><li><span><a href=\"#TBD:-Implementing-going-from-predictions-to-resultant-speedups\" data-toc-modified-id=\"TBD:-Implementing-going-from-predictions-to-resultant-speedups-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TBD: Implementing going from predictions to resultant speedups</a></span></li><li><span><a href=\"#TBD:-Implementing-KFold\" data-toc-modified-id=\"TBD:-Implementing-KFold-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TBD: Implementing KFold</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set editor width to something sane\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zacharias/ProGraML\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "# get root repository path\n",
    "a = !pwd\n",
    "REPO_ROOT = a[0].rsplit('ProGraML', maxsplit=1,)[0] + 'ProGraML'\n",
    "print(REPO_ROOT)\n",
    "#insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, REPO_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader, InMemoryDataset\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#from google.protobuf import text_format\n",
    "#from deeplearning.ml4pl.graphs import programl\n",
    "#from deeplearning.ml4pl.graphs.labelled import graph_tuple\n",
    "#from labm8.py import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplearning.ml4pl.graphs.unlabelled.llvm2graph import graph_builder\n",
    "\n",
    "builder = graph_builder.ProGraMLGraphBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev: Helper Functions\n",
    "`\n",
    "unreachable,232\n",
    "ret void,263\n",
    "!UNK,8564\n",
    "!IDENTIFIER, 8565\n",
    "magic/root/ I forgot, 8567\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx_graph = builder.Build(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev: nx2data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx2data(nx_graph, class_label=None):\n",
    "    r\"\"\"Converts a :obj:`networkx.Graph` or :obj:`networkx.DiGraph` to a\n",
    "    :class:`torch_geometric.data.Data` instance.\n",
    "\n",
    "    Args:\n",
    "        G (networkx.Graph or networkx.DiGraph): A networkx graph.\n",
    "    \"\"\"\n",
    "\n",
    "    # make sure the nx_graph is encoded properly (since node.x used to be buggy!)\n",
    "    # encoder = GraphNodeEncoder()\n",
    "    # encoder.EncodeNodes(nx_graph)\n",
    "\n",
    "    # collect edge_index\n",
    "    edge_index = torch.tensor(list(nx_graph.edges())).t().contiguous()\n",
    "\n",
    "    # collect edge_attr\n",
    "    positions = []\n",
    "    flows = []\n",
    "\n",
    "    for i, (_, _, edge_data) in enumerate(nx_graph.edges(data=True)):\n",
    "        positions.append(edge_data['position'])\n",
    "        flows.append(edge_data['flow'])\n",
    "\n",
    "    positions = torch.tensor(positions)\n",
    "    flows = torch.tensor(flows)\n",
    "\n",
    "    edge_attr = torch.cat([flows, positions]).view(2, -1).t().contiguous()\n",
    "    \n",
    "    # collect x\n",
    "    types = []\n",
    "    xs = []\n",
    "    \n",
    "    for i, node_data in nx_graph.nodes(data=True):\n",
    "        types.append(node_data['type'])\n",
    "        xs.append(node_data['x'][0])\n",
    "\n",
    "    xs = torch.tensor(xs)\n",
    "    types = torch.tensor(types)\n",
    "    \n",
    "    x = torch.cat([xs, types]).view(2, -1).t().contiguous()\n",
    "\n",
    "    \n",
    "    assert edge_attr.size()[0] == edge_index.size()[1], f'edge_attr={edge_attr.size()} size mismatch with edge_index={edge_index.size()}'\n",
    "    \n",
    "    if class_label is not None:\n",
    "        y = torch.tensor(int(class_label)).view(1)  # <1>\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    else:\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "def dump(outfile, data, mkdir=True):\n",
    "    if mkdir:\n",
    "        outfile.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(outfile, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def process_single_folder(folder, dump_nx=True, dump_data=True):    \n",
    "    problems = \"\"\n",
    "    out_base = folder.parent\n",
    "    \n",
    "    print(f\"=== Opening Folder {str(folder)} ===\")\n",
    "    \n",
    "    #label = int(folder.name)\n",
    "    # TODO: generate labels!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    label = None\n",
    "    \n",
    "    \n",
    "    files = list(folder.glob('*.ll'))\n",
    "    \n",
    "    # iterate over all .ll files in folder and confirm and respectively create the .nx.p and .data.p files\n",
    "    for i, file in enumerate(files):\n",
    "        outfile_nx = out_base / (folder.name + '_programl') / '_nx' / (file.name.rsplit('.', 1)[0] + '.nx.p')\n",
    "        outfile_data = out_base / (folder.name + '_programl') / (file.name.rsplit('.', 1)[0] + '.data.p')\n",
    "\n",
    "        # find out where to start processing\n",
    "\n",
    "        # skip entirely?\n",
    "        if outfile_data.is_file():\n",
    "            continue\n",
    "\n",
    "        # start at step 2: nx --> data ?\n",
    "        if outfile_nx.is_file():\n",
    "            nx_graph = load(outfile_nx)            \n",
    "            data = nx2data(nx_graph, class_label=label)\n",
    "            dump(outfile_data, data)\n",
    "            continue\n",
    "        \n",
    "        # start in the beginning:\n",
    "        # ~~~ step 1: .ll --> nx ~~~\n",
    "        if i % 100 == 0:\n",
    "            print(f\"{folder.name} - [{i}/{len(files)}] Processing {str(file)} ...\")\n",
    "        \n",
    "        with open(file, 'r') as f:\n",
    "            bytecode = f.read()\n",
    "\n",
    "        try:\n",
    "            nx_graph = builder.Build(bytecode) # nx\n",
    "            if dump_nx:\n",
    "                dump(outfile_nx, nx_graph)\n",
    "        except:\n",
    "            print(f\"***** FAILING ON {str(file)} ... renaming file to .ll_ \")\n",
    "            problems += str(file)\n",
    "            problems += '\\n'\n",
    "            file.rename(file.with_suffix('.ll_'))\n",
    "            continue\n",
    "\n",
    "        # step 2: nx --> data\n",
    "        data = nx2data(nx_graph, class_label=label)\n",
    "        dump(outfile_data, data)\n",
    "\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# download dataset if needed (this is for classifyapp!)\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "def download_and_unzip(url, dataset_name, data_folder):\n",
    "    \"\"\"\n",
    "    Download and unzip data set folder from url\n",
    "    :param url: from which to download\n",
    "    :param dataset_name: name of data set (for printing)\n",
    "    :param data_folder: folder in which to put the downloaded data\n",
    "    \"\"\"\n",
    "    print('Downloading', dataset_name, 'data set...')\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    data_zip = wget.download(url, out=data_folder)\n",
    "    print('\\tunzipping...')\n",
    "    zip_ = zipfile.ZipFile(data_zip, 'r')\n",
    "    assert os.path.isdir(data_folder), data_folder\n",
    "    zip_.extractall(data_folder)\n",
    "    zip_.close()\n",
    "    print('\\tdone')\n",
    "\n",
    "def download_classifyapp(dataset_path):\n",
    "    # get Path object\n",
    "    if type(dataset_path) == str:\n",
    "        dataset_path = Path(dataset_path)\n",
    "    dataset_path = dataset_path / 'classifyapp_data'\n",
    "        \n",
    "    # Acquire data\n",
    "    if not dataset_path.exists():\n",
    "        dataset_path.mkdir(parents=True)\n",
    "        download_and_unzip('https://polybox.ethz.ch/index.php/s/JOBjrfmAjOeWCyl/download',\n",
    "                                      'classifyapp_data', str(dataset_path.absolute()))\n",
    "    else:\n",
    "        print(f'skipped downloading to {str(dataset_path.absolute())}')\n",
    "        \n",
    "        \n",
    "\n",
    "def download_threadcoarsening(dataset_path):\n",
    "    # get Path object\n",
    "    if type(dataset_path) == str:\n",
    "        dataset_path = Path(dataset_path)\n",
    "    dataset_path = dataset_path / 'threadcoarsening_data'\n",
    "        \n",
    "    # Acquire data\n",
    "    if not dataset_path.exists():\n",
    "        dataset_path.mkdir(parents=True)\n",
    "        download_and_unzip('https://polybox.ethz.ch/index.php/s/Dl8v8dKbuoWS3Ck/download',\n",
    "                                      'threadcoarsening_data',  str(dataset_path.absolute()))\n",
    "    else:\n",
    "        print(f'skipped downloading to {str(dataset_path.absolute())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "## *needs action:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download threadcoarsening\n",
    "dataset_name = 'threadcoarsening'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zacharias/ProGraML/deeplearning/ml4pl/poj104\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading threadcoarsening_data data set...\n",
      "\tunzipping...\n",
      "\tdone\n"
     ]
    }
   ],
   "source": [
    "# Set where to store the dataset and download automagically\n",
    "ds_basepath = Path('/mnt/data/llvm/master_thesis_datasets')\n",
    "\n",
    "# uncomment this line to save data \"in place\"\n",
    "#ds_basepath = Path(repo_root) / 'deeplearning/ml4pl/poj104'\n",
    "\n",
    "logs_basepath = ds_basepath / 'logs' / f'{dataset_name}_logs'\n",
    "\n",
    "ds_basepath.mkdir(parents=True, exist_ok=True)\n",
    "logs_basepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#download threadcoarsening\n",
    "download_threadcoarsening(ds_basepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/llvm/master_thesis_datasets/threadcoarsening_data\n",
      "/home/zacharias/ProGraML/deeplearning/ml4pl/poj104/\n",
      "/mnt/data/llvm/master_thesis_datasets/logs/threadcoarsening_logs\n",
      "/home/zacharias/ProGraML/deeplearning/ml4pl/poj104/\n"
     ]
    }
   ],
   "source": [
    "# link those places into poj104 folder\n",
    "\n",
    "data_source = str((ds_basepath / f'{dataset_name}_data').absolute())\n",
    "print(data_source)\n",
    "data_target = REPO_ROOT + '/deeplearning/ml4pl/poj104/'\n",
    "print(data_target)\n",
    "\n",
    "logs_source = str(logs_basepath.absolute())\n",
    "print(logs_source)\n",
    "logs_target = REPO_ROOT + '/deeplearning/ml4pl/poj104/'\n",
    "print(logs_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/home/zacharias/ProGraML/deeplearning/ml4pl/poj104/threadcoarsening_data': File exists\n",
      "ln: failed to create symbolic link '/home/zacharias/ProGraML/deeplearning/ml4pl/poj104/threadcoarsening_logs': File exists\n",
      "lrwxrwxrwx  1 zacharias zacharias   59 Feb  2 12:12 threadcoarsening_data -> /mnt/data/llvm/master_thesis_datasets/threadcoarsening_data\n",
      "lrwxrwxrwx  1 zacharias zacharias   64 Feb  2 12:12 threadcoarsening_logs -> /mnt/data/llvm/master_thesis_datasets/logs/threadcoarsening_logs\n"
     ]
    }
   ],
   "source": [
    "! ln -s {data_source} {data_target}\n",
    "! ln -s {logs_source} {logs_target}\n",
    "! ls -lah {str(REPO_ROOT + '/deeplearning/ml4pl/poj104')} | grep {dataset_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process .ll files (from NCC release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernels_ir\n",
      "/mnt/data/llvm/master_thesis_datasets/threadcoarsening_data/kernels_ir\n"
     ]
    }
   ],
   "source": [
    "data_source = Path(data_source)\n",
    "dataset_path = data_source / 'kernels_ir'\n",
    "print(dataset_path.name)\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Opening Folder /mnt/data/llvm/master_thesis_datasets/threadcoarsening_data/kernels_ir ===\n",
      "kernels_ir - [0/17] Processing /mnt/data/llvm/master_thesis_datasets/threadcoarsening_data/kernels_ir/sgemm.ll ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_single_folder(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetch labels etc. from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset requirements for training:\n",
    "\n",
    "* Leave one out cross validation per plattform.\n",
    "* Not all coarsening factors have runtimes, so if model predicts \"too high\" cf, we need to \"clamp it down\" to the next highest existing one to compute runtimes.\n",
    "\n",
    "```\n",
    "\n",
    "    # The runtimes of some coarsening factors are not recorded in the data table. If that is the case for\n",
    "    # the predicted cf, clamp it down to the highest cf for which the runtime is recorded\n",
    "    p = min(p, 2 ** (len(X_cc[test_index[0]]) - 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def platform2str(platform):\n",
    "    if platform == \"Fermi\":\n",
    "        return \"NVIDIA GTX 480\"\n",
    "    elif platform == \"Kepler\":\n",
    "        return \"NVIDIA Tesla K20c\"\n",
    "    elif platform == \"Cypress\":\n",
    "        return \"AMD Radeon HD 5900\"\n",
    "    elif platform == \"Tahiti\":\n",
    "        return \"AMD Tahiti 7970\"\n",
    "    else:\n",
    "        raise LookupError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_runtimes(platform, df, oracles):\n",
    "    all_runtimes = {}\n",
    "    for kernel in oracles['kernel']:\n",
    "        kernel_r = []\n",
    "        for cf in [1, 2, 4, 8, 16, 32]:\n",
    "            row = df[(df['kernel'] == kernel) & (df['cf'] == cf)]\n",
    "            if len(row) == 1:\n",
    "                kernel_r.append(float(row[f'runtime_{platform}'].values))\n",
    "            elif len(row) == 0:\n",
    "                print(f' kernel={kernel:>20} is missing cf={cf}. Ad-hoc inserting last existing cf!')\n",
    "                kernel_r.append(kernel_r[-1])\n",
    "            else:\n",
    "                raise\n",
    "        all_runtimes[kernel] = kernel_r\n",
    "    return all_runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set values:\n",
    "platform = \"Cypress\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read runtime info\n",
    "oracle_file = data_source / \"pact-2014-oracles.csv\"\n",
    "oracles = pd.read_csv(oracle_file)\n",
    "\n",
    "runtimes_file = data_source / \"pact-2014-runtimes.csv\"\n",
    "df = pd.read_csv(runtimes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kernel=        binarySearch is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=        blackscholes is missing cf=16. Ad-hoc inserting last existing cf!\n",
      " kernel=        blackscholes is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=         convolution is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=           dwtHaar1D is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=           fastWalsh is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=                mriQ is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=              mvCoal is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=            mvUncoal is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=               nbody is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=              reduce is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=                spmv is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=             stencil is missing cf=32. Ad-hoc inserting last existing cf!\n"
     ]
    }
   ],
   "source": [
    "# get nice runtimes dict\n",
    "runtimes_dict = get_all_runtimes(platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get oracle labels\n",
    "y = np.array([cfs.index(int(x)) for x in oracles[\"cf_\" + platform]], dtype=np.int64)\n",
    "\n",
    "# sanity check\n",
    "for i, (k, v) in enumerate(runtimes_dict.items()):\n",
    "    assert int(y[i]) == np.argmin(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_attr=[131, 2], edge_index=[2, 131], runtimes=[6], x=[75, 2], y=[])\n",
      "Data(edge_attr=[233, 2], edge_index=[2, 233], runtimes=[6], x=[137, 2], y=[])\n",
      "Data(edge_attr=[376, 2], edge_index=[2, 376], runtimes=[6], x=[209, 2], y=[])\n",
      "Data(edge_attr=[343, 2], edge_index=[2, 343], runtimes=[6], x=[188, 2], y=[])\n",
      "Data(edge_attr=[70, 2], edge_index=[2, 70], runtimes=[6], x=[43, 2], y=[])\n",
      "Data(edge_attr=[94, 2], edge_index=[2, 94], runtimes=[6], x=[58, 2], y=[])\n",
      "Data(edge_attr=[225, 2], edge_index=[2, 225], runtimes=[6], x=[128, 2], y=[])\n",
      "Data(edge_attr=[465, 2], edge_index=[2, 465], runtimes=[6], x=[242, 2], y=[])\n",
      "Data(edge_attr=[465, 2], edge_index=[2, 465], runtimes=[6], x=[242, 2], y=[])\n",
      "Data(edge_attr=[265, 2], edge_index=[2, 265], runtimes=[6], x=[148, 2], y=[])\n",
      "Data(edge_attr=[286, 2], edge_index=[2, 286], runtimes=[6], x=[158, 2], y=[])\n",
      "Data(edge_attr=[1104, 2], edge_index=[2, 1104], runtimes=[6], x=[528, 2], y=[])\n",
      "Data(edge_attr=[183, 2], edge_index=[2, 183], runtimes=[6], x=[107, 2], y=[])\n",
      "Data(edge_attr=[280, 2], edge_index=[2, 280], runtimes=[6], x=[159, 2], y=[])\n",
      "Data(edge_attr=[374, 2], edge_index=[2, 374], runtimes=[6], x=[196, 2], y=[])\n",
      "Data(edge_attr=[290, 2], edge_index=[2, 290], runtimes=[6], x=[171, 2], y=[])\n",
      "Data(edge_attr=[272, 2], edge_index=[2, 272], runtimes=[6], x=[157, 2], y=[])\n"
     ]
    }
   ],
   "source": [
    "# load graphs and add attributes\n",
    "data_list = []\n",
    "\n",
    "kernels = oracles[\"kernel\"].values  # list of strings of kernel names\n",
    "\n",
    "for kernel in kernels:\n",
    "    file = data_source / 'kernels_ir_programl' / (kernel + '.data.p')\n",
    "    assert file.exists(), f'input file not found: {file}'\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    # add attributes\n",
    "    data['y'] = torch.tensor(np.argmin(runtimes_dict[kernel]), dtype=torch.long)\n",
    "    data['runtimes'] = torch.tensor(runtimes_dict[kernel])\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cross validation step [ 1 /  17 ]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kernel=        binarySearch is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=        blackscholes is missing cf=16. Ad-hoc inserting last existing cf!\n",
      " kernel=        blackscholes is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=         convolution is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=           dwtHaar1D is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=           fastWalsh is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=                mriQ is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=              mvCoal is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=            mvUncoal is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=               nbody is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=              reduce is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=                spmv is missing cf=32. Ad-hoc inserting last existing cf!\n",
      " kernel=             stencil is missing cf=32. Ad-hoc inserting last existing cf!\n"
     ]
    }
   ],
   "source": [
    "platform = 'Fermi'\n",
    "\n",
    "cfs = [1, 2, 4, 8, 16, 32]  # thread coarsening factors\n",
    "\n",
    "device_list = [\"Cypress\", \"Tahiti\", \"Fermi\", \"Kepler\"]\n",
    "\n",
    "oracle_runtimes = np.array([float(x) for x in oracles[\"runtime_\" + platform]])\n",
    "#y = np.array([int(x) for x in oracles[\"cf_\" + platform]], dtype=np.int64)\n",
    "y = np.array([cfs.index(int(x)) for x in oracles[\"cf_\" + platform]], dtype=np.int64)\n",
    "#y_1hot = get_onehot(oracles, platform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBD: Implementing going from predictions to resultant speedups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBD: Implementing KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cross validation step [ 1 /  17 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=len(y), shuffle=False)\n",
    "for j, (train_index, test_index) in enumerate(kf.split(y)):\n",
    "    print('--- Cross validation step [', j+1, '/ ', len(y), ']')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "762.997px",
    "width": "260px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
