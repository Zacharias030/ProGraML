{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dense-vs-Sparse-Self-Attention\" data-toc-modified-id=\"Dense-vs-Sparse-Self-Attention-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Dense vs Sparse Self-Attention</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-equality-of-sparse-implementation-with-reference\" data-toc-modified-id=\"Check-equality-of-sparse-implementation-with-reference-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Check equality of sparse implementation with reference</a></span></li><li><span><a href=\"#Check-equality-of-sparse-implementation-with-reference\" data-toc-modified-id=\"Check-equality-of-sparse-implementation-with-reference-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Check equality of sparse implementation with reference</a></span></li></ul></li><li><span><a href=\"#Benchmarking-Sparse-vs-Dense-Self-Attention\" data-toc-modified-id=\"Benchmarking-Sparse-vs-Dense-Self-Attention-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Benchmarking Sparse vs Dense Self-Attention</a></span></li><li><span><a href=\"#scatter_add-vs-index_add\" data-toc-modified-id=\"scatter_add-vs-index_add-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>scatter_add vs index_add</a></span></li><li><span><a href=\"#embedding-lookup-vs-index_select\" data-toc-modified-id=\"embedding-lookup-vs-index_select-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>embedding lookup vs index_select</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scattered-GRU-input\" data-toc-modified-id=\"Scattered-GRU-input-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Scattered GRU input</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "\n",
    "# make this file executable from anywhere\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "# in editor\n",
    "#full_path = os.path.realpath(__file__)\n",
    "\n",
    "# in jupyter\n",
    "full_path = !pwd\n",
    "full_path = full_path[0]\n",
    "\n",
    "\n",
    "print(full_path)\n",
    "REPO_ROOT = full_path.rsplit('ProGraML', maxsplit=1)[0] + 'ProGraML'\n",
    "print(REPO_ROOT)\n",
    "#insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, REPO_ROOT)\n",
    "REPO_ROOT = Path(REPO_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric import utils\n",
    "from torch_geometric.utils import softmax as scatter_softmax\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplearning.ml4pl.models.ggnn.modeling import SelfAttentionMessageLayer\n",
    "from deeplearning.ml4pl.models.ggnn.configs import BaseConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense vs Sparse Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_dict(mod):\n",
    "    for n, t in mod.state_dict().items():\n",
    "        print(n, t.size())\n",
    "\n",
    "def num_parameters(mod) -> int:\n",
    "    \"\"\"Compute the number of trainable parameters in a nn.Module and its children.\"\"\"\n",
    "    num_params = sum(param.numel() for param in mod.parameters(recurse=True) if param.requires_grad)\n",
    "    return num_params, f\"{num_params * 4 / 1e6:.3f}MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BaseConfig.from_dict({\n",
    "    'edge_type_count': 3,\n",
    "    'backward_edges': True,\n",
    "    'hidden_size': 200,\n",
    "    'transformer_attn_bias': True,\n",
    "    'transformer_num_heads': 8,\n",
    "    'transformer_attn_dropout': 0.0,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check equality of sparse implementation with reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_attn = SelfAttentionMessageLayer(config).to(device='cuda')\n",
    "\n",
    "#print_state_dict(sparse_attn)\n",
    "#print(\"\")\n",
    "#print(sparse_attn)\n",
    "#print(\"\")\n",
    "#num_parameters(sparse_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummy data\n",
    "\n",
    "num_n = 5\n",
    "heads = 8\n",
    "hidd = 200\n",
    "\n",
    "random_emb = nn.Parameter(torch.randn(9000, hidd, device='cuda'), requires_grad=True)\n",
    "\n",
    "ones = torch.ones(num_n, num_n, device='cuda')\n",
    "\n",
    "adj = torch.tril(ones, diagonal=0)\n",
    "#adj = ones\n",
    "print(\"adj matrix: edges go from (row -> column) if [row, column] == 1\")\n",
    "print(adj)\n",
    "\n",
    "attn_mask = torch.tril(ones * float('-inf'), diagonal=-1)\n",
    "\n",
    "print(\"\")\n",
    "print(\"attn_mask.t(): We print the transposed attention mask,\")\n",
    "print(\"bc. the reference implementation indexes the attn_mask <target, src>\")\n",
    "print(attn_mask.t())\n",
    "\n",
    "edge_index = utils.dense_to_sparse(adj)[0]\n",
    "print(edge_index.size())\n",
    "#print(edge_index.t()[:10])\n",
    "\n",
    "x = torch.randint(9000, (num_n,))\n",
    "print(x.size())\n",
    "\n",
    "node_states = random_emb[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sparse attn on dummy data\n",
    "sa_node_states, sa_weights = sparse_attn(edges=edge_index, node_states=node_states)\n",
    "sa_sum = torch.sum(sa_node_states)\n",
    "\n",
    "print(sa_sum)\n",
    "\n",
    "\n",
    "in_proj = sparse_attn.qkv_in_proj.weight\n",
    "in_proj_bias = sparse_attn.qkv_in_proj.bias\n",
    "out_proj = sparse_attn.out_proj.weight\n",
    "out_proj_bias = sparse_attn.out_proj.bias\n",
    "\n",
    "\n",
    "ns = node_states.unsqueeze(1)\n",
    "da_node_states, da_weights = F.multi_head_attention_forward(\n",
    "                ns, ns, ns, 200, 8,\n",
    "                in_proj, in_proj_bias,\n",
    "                None, None, False,\n",
    "                0.0, out_proj, out_proj_bias,\n",
    "                training=False,\n",
    "                key_padding_mask=None, need_weights=True, \n",
    "                attn_mask=attn_mask)\n",
    "da_sum = torch.sum(da_node_states)\n",
    "\n",
    "\n",
    "# reformat\n",
    "da_node_states = da_node_states.squeeze()\n",
    "da_weights = da_weights.squeeze().t()\n",
    "\n",
    "print(da_sum)\n",
    "\n",
    "print(da_weights)\n",
    "\n",
    "\n",
    "sa_weights_matrix = torch.zeros(num_n, num_n, device='cuda')\n",
    "for i, (s, t) in enumerate(edge_index.t()):\n",
    "    sa_weights_matrix[s, t] = sa_weights[i]\n",
    "print(sa_weights_matrix)\n",
    "#print(sa_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert similarity of sums of outputs\n",
    "check1 = torch.abs(sa_sum - da_sum)\n",
    "print((check1 < 1e-7).item())\n",
    "print(torch.max(check1).item())\n",
    "# assert similarity of attention weights for complete graphs\n",
    "check2 = torch.abs(da_weights - sa_weights_matrix )\n",
    "print(torch.all(check2 < 1e-7).item())\n",
    "print(torch.max(check2).item())\n",
    "\n",
    "# assert similarity of self-attention output (new node states)\n",
    "check3 = torch.abs(da_node_states - sa_node_states)\n",
    "print(torch.all(check3 < 1e-7).item())\n",
    "print(torch.max(check3).item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Sparse vs Dense Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def satt(x, emb_table, edge_index):\n",
    "    node_states = emb_table[x]\n",
    "    s = sparse_attn(edges=edge_index, node_states=node_states)\n",
    "    loss = torch.sum(s[0])\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "def datt(x, emb_table, attn_mask):\n",
    "    query = emb_table[x.unsqueeze(1)]\n",
    "    s = dense_attn(query, query, query, attn_mask=attn_mask, need_weights=False)\n",
    "    loss = torch.sum(s[0])\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "def benchmark_on_causal(s,n):\n",
    "    for num_n in [2**i for i in range(s,n+1)]:\n",
    "        print(num_n)\n",
    "        with torch.no_grad():\n",
    "            causal_mask = (torch.triu(torch.ones(num_n, num_n, device='cuda')) - 1) * 1e4\n",
    "            edge_index = utils.dense_to_sparse(causal_mask)[0]\n",
    "            print(edge_index.size())\n",
    "            x = torch.randint(9000, (num_n,))\n",
    "        torch.cuda.synchronize()\n",
    "        #print(\"sparse\")\n",
    "        %timeit satt(x, random_emb, edge_index)\n",
    "        \n",
    "        #print(\"dense\")\n",
    "        try:\n",
    "            %timeit datt(x, random_emb, causal_mask)\n",
    "        except RuntimeError:\n",
    "            print(\"Dense OOM\")\n",
    "        del causal_mask\n",
    "        del edge_index\n",
    "        del x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_on_causal(8, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real data\n",
    "from deeplearning.ml4pl.poj104.dataset import POJ104Dataset\n",
    "ds_dir = repo_root / 'deeplearning/ml4pl/poj104/classifyapp_data'\n",
    "valid_data = POJ104Dataset(ds_dir, 'val')\n",
    "\n",
    "\n",
    "def benchmark_on_real(s,n):\n",
    "    for bsz in [2**i for i in range(s,n+1)]:\n",
    "        dl = DataLoader(valid_data, batch_size=bsz)\n",
    "        print(bsz)\n",
    "        for graph in dl:\n",
    "            graph.to(device='cuda')\n",
    "            print(graph)\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = graph.x[:,0]\n",
    "            edge_index = graph.edge_index\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        %timeit satt(x, random_emb, edge_index)\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                mask = (1 - utils.to_dense_adj(graph.edge_index).squeeze() ) * -10000\n",
    "                print(f\"Size of adjacency matrix: {mask.size()[0]**2 * 4 / 1e6:,.2f} MB\")\n",
    "            torch.cuda.synchronize()    \n",
    "            %timeit datt(x, random_emb, mask)\n",
    "        except RuntimeError:\n",
    "            print(\"Dense OOM\")\n",
    "            \n",
    "        print(\"----\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "benchmark_on_real(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(valid_data, batch_size=64)\n",
    "for graph in dl:\n",
    "    graph.to(device='cuda')\n",
    "    break\n",
    "print(graph)\n",
    "\n",
    "with torch.no_grad():\n",
    "    adj = (1 - to_dense_adj(graph.edge_index).squeeze() ) * -10000\n",
    "adj.size(), adj.device\n",
    "print(f\"Size of adjacency matrix: {adj.size()[0]**2 * 4 / 1e6:,.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# scatter_add vs index_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target = torch.randn(2000,200, requires_grad=True, device='cuda')\n",
    "mbs = torch.randn(10000, 200, device='cuda', requires_grad=True)\n",
    "index = torch.randint(0, 2000, [10000], device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sa = torch.zeros_like(target)\n",
    "sa.scatter_add_(0, index.unsqueeze(1).repeat(1, 200), mbs)\n",
    "\n",
    "s = torch.zeros_like(target)\n",
    "s.index_add_(0, index, mbs)\n",
    "\n",
    "torch.max(torch.abs(sa - s)), torch.argmax(torch.abs(sa - s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ia():\n",
    "    s = torch.zeros_like(target)\n",
    "    s.index_add_(0, index, mbs)\n",
    "    loss = torch.sum(s)\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "def sa():\n",
    "    s = torch.zeros_like(target)\n",
    "    s.scatter_add_(0, index.unsqueeze(1).repeat(1, 200), mbs)\n",
    "    loss = torch.sum(s)\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    \n",
    "torch.cuda.synchronize()    \n",
    "%timeit ia()\n",
    "%timeit sa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# embedding lookup vs index_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "bs = 2000\n",
    "mask = torch.randn(bs, device='cuda')\n",
    "mask2 = mask>0\n",
    "index = torch.randint(0, 2000, [10000], dtype=torch.long, device='cuda')\n",
    "\n",
    "target = torch.randn(bs,200, requires_grad=True, device='cuda')\n",
    "pred = torch.randn(bs,200, requires_grad=True, device='cuda')\n",
    "\n",
    "h = torch.nn.GRUCell(200, 200).to(device='cuda')\n",
    "g = torch.nn.GRUCell(200, 200).to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.equal(F.embedding(index, target), torch.index_select(target, dim=0, index=index))\n",
    "torch.index_select(target, dim=0, index=index).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def emb():\n",
    "    selected = F.embedding(index, target)\n",
    "    loss = torch.sum(selected)\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "def index_select():\n",
    "    selected = torch.index_select(target, dim=0, index=index)\n",
    "    loss = torch.sum(selected)\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "%timeit emb()\n",
    "%timeit index_select()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Scattered GRU input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fn1():\n",
    "    a = torch.zeros_like(target,device='cuda')\n",
    "    a[mask2] += g(target[mask2])\n",
    "    a[~mask2] += h(target[~mask2])\n",
    "    \n",
    "    loss = torch.sum(a)\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "    return loss\n",
    "\n",
    "l = fn1()\n",
    "\n",
    "def fn2():\n",
    "    a = torch.zeros_like(target,device='cuda')\n",
    "    a += mask2.view(-1, 1) * g(target)\n",
    "    a += ~mask2.view(-1, 1) * h(target)\n",
    "    loss = torch.sum(a)\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "    return loss\n",
    "\n",
    "ll = fn2()\n",
    "    \n",
    "torch.cuda.synchronize()\n",
    "\n",
    "%timeit fn1()\n",
    "%timeit fn2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(ll)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fn1():\n",
    "    loss = torch.sum(mask.view(-1, 1) * (target - pred) ** 2)\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "def fn2():\n",
    "    loss = torch.sum((target[mask2,:] - pred[mask2,:]) ** 2)\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "%timeit fn1()\n",
    "%timeit fn2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
