{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Failure Cases of the Network <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-&amp;-Config\" data-toc-modified-id=\"Imports-&amp;-Config-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports &amp; Config</a></span></li><li><span><a href=\"#Restore-Run-for-Analysis\" data-toc-modified-id=\"Restore-Run-for-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Restore Run for Analysis</a></span></li><li><span><a href=\"#Establish-Test-Set-Accuracy\" data-toc-modified-id=\"Establish-Test-Set-Accuracy-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Establish Test Set Accuracy</a></span></li><li><span><a href=\"#Analyze-Model-Predictions-on-Test-Set\" data-toc-modified-id=\"Analyze-Model-Predictions-on-Test-Set-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Analyze Model Predictions on Test Set</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-Confusion-Matrix\" data-toc-modified-id=\"Simple-Confusion-Matrix-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Simple Confusion Matrix</a></span></li><li><span><a href=\"#Fancy-Confusion-Matrix\" data-toc-modified-id=\"Fancy-Confusion-Matrix-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Fancy Confusion Matrix</a></span></li></ul></li><li><span><a href=\"#Token-Distribution-on-Failure-Cases\" data-toc-modified-id=\"Token-Distribution-on-Failure-Cases-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Token Distribution on Failure Cases</a></span><ul class=\"toc-item\"><li><span><a href=\"#Failure-Cases\" data-toc-modified-id=\"Failure-Cases-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Failure Cases</a></span></li><li><span><a href=\"#Good-Cases\" data-toc-modified-id=\"Good-Cases-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Good Cases</a></span></li></ul></li><li><span><a href=\"#Analyze-Dataset-Statistics\" data-toc-modified-id=\"Analyze-Dataset-Statistics-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Analyze Dataset Statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Validation-Set\" data-toc-modified-id=\"Validation-Set-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Validation Set</a></span></li><li><span><a href=\"#Notable-tokens-and-their-statements\" data-toc-modified-id=\"Notable-tokens-and-their-statements-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Notable tokens and their statements</a></span></li><li><span><a href=\"#Train-Set\" data-toc-modified-id=\"Train-Set-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Train Set</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports \\& Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set editor width to something sane\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# get root repository path\n",
    "a = !pwd\n",
    "repo_root = a[0].rsplit('ProGraML', maxsplit=1,)[0] + 'ProGraML'\n",
    "print(repo_root)\n",
    "#insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, repo_root)\n",
    "repo_root = Path(repo_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplearning.ml4pl.models.ggnn.ggnn_config_poj104 import GGNNConfig\n",
    "from deeplearning.ml4pl.models.ggnn.run_ggnn_poj104 import Learner\n",
    "from deeplearning.ml4pl.models.ggnn.ggnn_modules import GGNNModel\n",
    "from deeplearning.ml4pl.poj104.dataset import POJ104Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore Run for Analysis\n",
    "\n",
    "Set your custom `run_to_analyze` to something like `'2020-01-16-16-21-58_29971'` below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set run to analyze\n",
    "run_to_analyze = '2020-01-16-16-21-58_29971'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls classifyapp_logs/ | grep {run_to_analyze}\n",
    "print(\"\")\n",
    "!cat classifyapp_logs/{run_to_analyze}_params.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Analyzer(object):\n",
    "    \"\"\"restore a model and analyze it deeply.\"\"\"\n",
    "    def __init__(self, args):\n",
    "        # set default args and update\n",
    "        self.args = {\n",
    "            '--data_dir': 'deeplearning/ml4pl/poj104/classifyapp_data',\n",
    "            '--help': False,\n",
    "            '--log_dir': 'deeplearning/ml4pl/poj104/classifyapp_logs/',\n",
    "            '--restore': None,\n",
    "            '--skip_restore_config': False,\n",
    "            '--test': False,\n",
    "        }\n",
    "        self.args.update(args)\n",
    "        print(\"initialized with:\")\n",
    "        print(self.args)\n",
    "\n",
    "        if self.args.get('--restore', None) is not None:\n",
    "            self.model = self.restore_model(path=repo_root / self.args['--restore'])\n",
    "        else: # initialize fresh model\n",
    "            self.global_training_step = 0\n",
    "            self.current_epoch = 1\n",
    "            test_only = self.args.get('--test', False)\n",
    "            self.model = GGNNModel(self.config, test_only=test_only)\n",
    "        \n",
    "        # load data\n",
    "        self.data_dir = repo_root / self.args.get('--data_dir', '.')\n",
    "        self.valid_data = DataLoader(POJ104Dataset(root=self.data_dir, split='val'), batch_size=self.config.batch_size * 2, shuffle=False)\n",
    "        self.test_data = DataLoader(POJ104Dataset(root=self.data_dir, split='test'), batch_size=self.config.batch_size * 2, shuffle=False)\n",
    "        self.train_data = DataLoader(POJ104Dataset(root=self.data_dir, split='train', train_subset=[0,100]), batch_size=self.config.batch_size, shuffle=False)\n",
    "        \n",
    "        \n",
    "    def restore_model(self, path):\n",
    "        \"\"\"loads and restores a model from file.\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.run_id = checkpoint['run_id']\n",
    "        self.global_training_step = checkpoint['global_training_step']\n",
    "        self.current_epoch = checkpoint['epoch']\n",
    "\n",
    "        config_dict = checkpoint['config'] if isinstance(checkpoint['config'], dict) else checkpoint['config'].to_dict()\n",
    "        if not self.args.get('--skip_restore_config'):\n",
    "            config = GGNNConfig.from_dict(config_dict)\n",
    "            self.config = config\n",
    "            print(f'*RESTORED* self.config from checkpoint {str(path)}.')\n",
    "        else:\n",
    "            print(f'Skipped restoring self.config from checkpoint!')\n",
    "            self.config.check_equal(config_dict)\n",
    "        \n",
    "        test_only = self.args.get('--test', False)\n",
    "        model = GGNNModel(self.config, test_only=test_only)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f'*RESTORED* model parameters from checkpoint {str(path)}.')\n",
    "        if not self.args.get('--test', None):  # only restore opt if needed. opt should be None o/w.\n",
    "            model.opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(f'*RESTORED* optimizer parameters from checkpoint as well.')\n",
    "        return model\n",
    "\n",
    "    def run_analysis_epoch(self, loader, epoch_type='eval'):\n",
    "        \"\"\"\n",
    "        Runs a test epoch with detailed outputs.\n",
    "        \n",
    "        args:\n",
    "            loader: a pytorch-geometric dataset loader,\n",
    "            epoch_type: 'train' or 'eval'\n",
    "        returns:\n",
    "            loss, accuracy, instance_per_second\n",
    "        \"\"\"\n",
    "\n",
    "        bar = tqdm.tqdm(total=len(loader) * loader.batch_size, smoothing=0.01, unit='inst')\n",
    "        \n",
    "        saved_outputs = []\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        accuracies = []\n",
    "        start_time = time.time()\n",
    "        processed_graphs = 0\n",
    "\n",
    "        for step, batch in enumerate(loader):\n",
    "            num_graphs = batch.batch[-1].item() + 1\n",
    "            processed_graphs += num_graphs\n",
    "\n",
    "            ######### prepare input\n",
    "            # move batch to gpu and prepare input tensors:\n",
    "            batch.to(self.model.dev)\n",
    "\n",
    "            edge_lists = []\n",
    "            edge_positions = [] if self.config.position_embeddings else None\n",
    "            for i in range(3):\n",
    "                # mask by edge type\n",
    "                mask = batch.edge_attr[:, 0].squeeze() == i # <M_i>\n",
    "                edge_list = batch.edge_index[:, mask].t()\n",
    "                edge_lists.append(edge_list)\n",
    "                \n",
    "                if self.config.position_embeddings:\n",
    "                    edge_pos = batch.edge_attr[mask, 1]\n",
    "                    edge_positions.append(edge_pos)\n",
    "\n",
    "\n",
    "            #############\n",
    "            # RUN MODEL FORWARD PASS\n",
    "\n",
    "            # enter correct mode of model\n",
    "            if epoch_type == \"train\":\n",
    "                self.global_training_step += 1\n",
    "\n",
    "                if not self.model.training:\n",
    "                    self.model.train()\n",
    "                outputs = self.model(\n",
    "                    vocab_ids=batch.x.squeeze(),\n",
    "                    labels=batch.y - 1, # labels start at 0!!!\n",
    "                    edge_lists=edge_lists,\n",
    "                    pos_lists=edge_positions,\n",
    "                    num_graphs=num_graphs,\n",
    "                    graph_nodes_list=batch.batch,\n",
    "                )\n",
    "            else:  # not TRAIN\n",
    "                if self.model.training:\n",
    "                    self.model.eval()\n",
    "                    self.model.opt.zero_grad()\n",
    "                with torch.no_grad():  # don't trace computation graph!\n",
    "                    outputs = self.model(\n",
    "                        vocab_ids=batch.x.squeeze(),\n",
    "                        labels=batch.y - 1,\n",
    "                        edge_lists=edge_lists,\n",
    "                        pos_lists=edge_positions,\n",
    "                        num_graphs=num_graphs,\n",
    "                        graph_nodes_list=batch.batch,\n",
    "                    )\n",
    "            \n",
    "            saved_outputs.append(outputs)\n",
    "            (\n",
    "                logits,\n",
    "                accuracy,\n",
    "                logits,\n",
    "                correct,\n",
    "                targets,\n",
    "                graph_features,\n",
    "                *unroll_stats,\n",
    "            ) = outputs\n",
    "\n",
    "            loss = self.model.loss((logits, graph_features), targets)\n",
    "            epoch_loss += loss.item() * num_graphs\n",
    "            accuracies.append(np.array(accuracy.item()) * num_graphs)\n",
    "\n",
    "            if epoch_type == \"train\":\n",
    "                loss.backward()\n",
    "                # TODO(github.com/ChrisCummins/ProGraML/issues/27):: Clip gradients\n",
    "                # (done). NB, pytorch clips by norm of the gradient of the model, while\n",
    "                # tf clips by norm of the grad of each tensor separately. Therefore we\n",
    "                # change default from 1.0 to 6.0.\n",
    "                # TODO(github.com/ChrisCummins/ProGraML/issues/27):: Anyway: Gradients\n",
    "                # shouldn't really be clipped if not necessary?\n",
    "                if self.model.config.clip_grad_norm > 0.0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), self.model.config.clip_grad_norm\n",
    "                    )\n",
    "                self.model.opt.step()\n",
    "                self.model.opt.zero_grad()\n",
    "\n",
    "            bar.set_postfix(loss=epoch_loss / processed_graphs, acc=np.sum(accuracies, axis=0) / processed_graphs)\n",
    "            bar.update(num_graphs)\n",
    "\n",
    "        bar.close()\n",
    "        mean_loss = epoch_loss / processed_graphs\n",
    "        accuracy = np.sum(accuracies, axis=0) / processed_graphs\n",
    "        instance_per_sec = processed_graphs / (time.time() - start_time)\n",
    "        return mean_loss, accuracy, instance_per_sec\n",
    "    \n",
    "    def data2input(self, batch):\n",
    "        num_graphs = batch.batch[-1].item() + 1\n",
    "\n",
    "        ######### prepare input\n",
    "        # move batch to gpu and prepare input tensors:\n",
    "        batch.to(self.model.dev)\n",
    "\n",
    "        edge_lists = []\n",
    "        edge_positions = [] if self.config.position_embeddings else None\n",
    "        for i in range(3):\n",
    "            # mask by edge type\n",
    "            mask = batch.edge_attr[:, 0].squeeze() == i # <M_i>\n",
    "            edge_list = batch.edge_index[:, mask].t()\n",
    "            edge_lists.append(edge_list)\n",
    "\n",
    "            if self.config.position_embeddings:\n",
    "                edge_pos = batch.edge_attr[mask, 1]\n",
    "                edge_positions.append(edge_pos)\n",
    "        inputs = {\n",
    "            \"vocab_ids\": batch.x[:,0].squeeze(),\n",
    "            \"labels\": batch.y - 1, # labels start at 0!!!\n",
    "            \"edge_lists\": edge_lists,\n",
    "            \"pos_lists\": edge_positions,\n",
    "            \"num_graphs\": num_graphs,\n",
    "            \"graph_nodes_list\": batch.batch,\n",
    "        }\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Test Set Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test epoch the standard way first to check performance and checkpoint compatibility\n",
    "!pwd\n",
    "!python ../models/ggnn/run_ggnn_poj104.py --restore deeplearning/ml4pl/poj104/classifyapp_logs/{run_to_analyze}_model_best.pickle --test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    '--restore': 'deeplearning/ml4pl/poj104/classifyapp_logs/' + run_to_analyze + '_model_best.pickle',\n",
    "    '--test': True,\n",
    "}\n",
    "analyzer = Analyzer(args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Model Predictions on Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.model.eval()\n",
    "#logits_list, accuracy_list, correct_list, targets_list = [], [], [], []\n",
    "targets_list = []\n",
    "predicted_list = []\n",
    "correct_list = []\n",
    "for i, batch in enumerate(analyzer.test_data):\n",
    "\n",
    "    inputs = analyzer.data2input(batch)\n",
    "    with torch.no_grad():\n",
    "        outputs = analyzer.model(**inputs)\n",
    "    outputs = [o.clone().detach().cpu() for o in outputs]\n",
    "    \n",
    "    (\n",
    "        logits,\n",
    "        accuracy,\n",
    "        _,\n",
    "        correct,\n",
    "        targets,\n",
    "        _,\n",
    "        *unroll_stats,\n",
    "    ) = outputs\n",
    "    \n",
    "    predicted_list.append(logits.argmax(dim=1))\n",
    "    targets_list.append(targets)\n",
    "    correct_list.append(correct)\n",
    "    \n",
    "    #if i == 3:\n",
    "    #    break\n",
    "\n",
    "predicted = torch.cat(predicted_list)\n",
    "targets = torch.cat(targets_list)\n",
    "correct = torch.cat(correct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert torch.all((predicted == targets) == correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(((predicted == targets).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "cm = sklearn.metrics.confusion_matrix(targets, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.matshow(cm, cmap=plt.get_cmap('Blues'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fancy Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8*4, 6*4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]) if cm[i,j]>0 else \"\",\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, list(range(104)), normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Distribution on Failure Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_cases = []\n",
    "good_cases = []\n",
    "for i, data in enumerate(analyzer.test_data.dataset):\n",
    "    if correct[i]:\n",
    "        good_cases.append(data)\n",
    "    else:\n",
    "        failure_cases.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_bins = torch.zeros(8569,dtype=torch.long)\n",
    "for data in failure_cases:\n",
    "    bc = torch.bincount(data.x.squeeze(dim=1), minlength=8569)\n",
    "    failure_bins += bc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_bins = failure_bins.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(failure_bins[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_bins = torch.zeros(8569,dtype=torch.long)\n",
    "for data in good_cases:\n",
    "    bc = torch.bincount(data.x.squeeze(dim=1), minlength=8569)\n",
    "    good_bins += bc\n",
    "good_bins = good_bins.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many bins are not zero?\n",
    "print(sum(failure_bins > 0))\n",
    "print(sum(good_bins > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('token, (failure count, good count)')\n",
    "for i, cs in enumerate(zip(failure_bins, good_bins)):\n",
    "    if cs[0] or cs[1]:\n",
    "        print(i, cs[0], cs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('token, (count)')\n",
    "for i, cs in enumerate(zip(failure_bins, good_bins)):\n",
    "    if cs[0] or cs[1]:\n",
    "        print(i, cs[0] + cs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_log = np.log1p(good_bins)\n",
    "bad_log = np.log1p(failure_bins)\n",
    "plt.figure(figsize=(64, 8))\n",
    "plt.plot(good_log, ls='--')\n",
    "plt.plot(bad_log, ls='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(64, 8))\n",
    "plt.plot(good_bins)\n",
    "plt.plot(failure_bins)\n",
    "#plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Dataset Statistics\n",
    "### Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notable tokens and their statements\n",
    "\n",
    "```\n",
    "unreachable,232\n",
    "ret void,263\n",
    "!UNK,8564\n",
    "!IDENTIFIER, 8565\n",
    "magic/root/ I forgot, 8567\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.valid_data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all nodes\n",
    "x = []\n",
    "for i, data in enumerate(analyzer.valid_data.dataset):\n",
    "    x.append(data.x)\n",
    "x = torch.cat(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token distribution\n",
    "\n",
    "bins = torch.bincount(x[:,0], minlength=8568)\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('token, count, percentage of whole')\n",
    "s = torch.sum(bins)\n",
    "unique = 0\n",
    "for i, c in enumerate(bins):\n",
    "    if c:\n",
    "        unique += 1\n",
    "        #print(f\"{i}  {c.item()}   {(c*100/s.float()).item():.3f}\")\n",
    "print(f\"No of unique tokens {unique}.\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(64, 4))\n",
    "plt.plot(bins) #, ls='')\n",
    "plt.grid(which='both', axis='y')\n",
    "plt.yscale('log')\n",
    "plt.yticks()\n",
    "#plt.plot(bad_log, ls='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(128, 4))\n",
    "plt.bar(range(len(bins)),np.log1p(bins))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.train_data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for i, data in enumerate(analyzer.train_data.dataset):\n",
    "    x.append(data.x.squeeze(dim=1))\n",
    "x = torch.cat(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = torch.bincount(x, minlength=8568)\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('token, count, perc')\n",
    "s = torch.sum(bins)\n",
    "for i, c in enumerate(bins):\n",
    "    if c:\n",
    "        print(f\"{i}  {c.item()}   {(c*100/s.float()).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "206.989px",
    "width": "311.989px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
